{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70143e74",
   "metadata": {},
   "source": [
    "## Classification and Regression Trees\n",
    "\n",
    "This chapter describes a flexible data-driven method that can be used for both classification (called *classification tree*) and prediction (called *regression tree*). Among the data-driven methods, trees are the most transparent and easy to interpret. Trees are based on separating records into subgroups by  creating splits on predictors. These splits create logical rules that are transparent and easily understandable, for example, \"IF Age < 55 AND Education > 12 THEN class = 1\". The resulting subgroups should be more homogeneous in terms of the outcome variable, thereby creating useful prediction or classification rules. We discuss the two key ideas underlying trees: *recursive partitioning* (for constructing the tree) and *pruning* (for cutting the tree back). In the context of tree construction, we also describe a few metrics of homogeneity that are popular in tree algorithms, for determining the homogeneity of the resulting subgroups of records. We explain that limiting tree size is a useful strategy for avoiding overfitting and show how it is done. We also describe alternative strategies for avoiding overfitting. As with other data-driven methods, trees require large amounts of data. However, once constructed, they are computationally cheap to deploy even on large samples. They also have other advantages such as being highly automated, robust to outliers, and able to handle missing values. In addition to prediction and classification, we describe how trees can be used for dimension reduction. Finally, we introduce *random forests* and *boosted trees*, which combine results from multiple trees to improve predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49f0429",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "In this chapter, we will use pandas for data handling, scikit-learn for the models, and matplotlib and pydotplus for visualization. We will also make use of the utility functions from the Python Utilities Functions Appendix. Use the following import statements for the Python code in this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a259ade7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f50b6ab",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "If one had to choose a classification technique that performs well across a wide range of situations without requiring much effort from the analyst while being readily understandable by the consumer of the analysis, a strong contender would be the tree methodology developed by Breiman et al. (1984). We discuss this classification procedure first, then in later sections we show how the procedure can be extended to prediction of a numerical outcome. The program that Breiman et al. created to implement these procedures was called CART (Classification And Regression Trees). A related procedure is called C4.5.\n",
    "\n",
    "What is a classification tree? The figure below shows a tree for classifying bank customers who receive a loan\n",
    "offer as either acceptors or nonacceptors, based on information such as their income, education level,\n",
    "and average credit card expenditure.\n",
    "\n",
    "![title](../images/example-of-tree-classifying-bank-customers-as-loan-acceptors-or-non-acceptors.png)\n",
    "\n",
    "### Tree Structure\n",
    "\n",
    "We have two types of nodes in a tree: decision (=splitting) nodes and terminal nodes. Nodes that have successors are called *decision nodes* because if we were to use a tree to classify a new record for which we knew only the values of the predictor variables, we would \"drop\" the record down the tree so that at each decision node, the appropriate branch is taken until we get to a node that has no successors. Such nodes are called the *terminal nodes* (or *leaves* of the tree), and represent the partitioning of the data by predictors.\n",
    "\n",
    "It is useful to note that the type of trees grown by Python's `DecisionTreeClassifier()` method, also known as CART or *binary trees*, have the property that the number of terminal nodes is exactly one more than the number of decision nodes. When using the `export_graphviz()` function from `scikit-learn`, nodes are represented as boxes. The\n",
    "function has a large number of arguments that allow controlling the final graph. We use the utility function `plotDecisionTree` from the utility module for plotting the graphs in this chapter. With the chosen settings, all nodes contain information about the number of records in that node (samples), the distribution of the classes, and the majority class of that node. In addition, we color the nodes by the average value of the node for regression or purity of node for classification.\n",
    "\n",
    "For decision nodes, the name of the predictor variable chosen for splitting and its splitting value appear at the top. Of the two child nodes connected below a decision node, the left box is for records that meet the splitting condition (\"True\"), while the right box is for records that do not meet it (\"False\").\n",
    "\n",
    "### Decision Rules\n",
    "\n",
    "One of the reasons that tree classifiers are very popular is that they provide easily understandable decision rules (at least if the trees are not too large). Consider the tree in the example. The *terminal* odes are colored orange or blue corresponding to a nonacceptor (0) or acceptor (1) classification. The condition at the top of each splitting node gives the predictor and its splitting value for the split (e.g. *Income* ≤ 110.5 in the top node). *samples=* shows the number of records in that node, and *values=* shows the counts of the two classes (0 and 1) in that node; the labels are only shown in the top node. This tree can easily be translated into a set of rules for classifying a bank customer. For example, the bottom-left node under the \"Family\" decision node in this tree gives us the following rule:\n",
    "\n",
    "    IF(Income > 110.5) AND (Education ≤ 1.5) AND (Family ≤ 2.5)\n",
    "    THEN Class = 0 (nonacceptor).\n",
    "\n",
    "### Classifying a New Record\n",
    "\n",
    "To classify a new record, it is \"dropped\" down the tree. When it has dropped all the way down to a terminal node, we can assign its class simply by taking a \"vote\" (or average, if the outcome is numerical) of all the training data that belonged to the terminal node when the tree was grown. The class with the highest vote is assigned to the new record. For instance, a new record reaching the leftmost terminal node in above figure, which has a majority of records that belong to the 0 class, would be classified as \"nonacceptor\". Alternatively, we can convert the number of class 0 records in the node to a proportion (propensity) and then compare the proportion to a user-specified cutoff value. In a binary classification situation (typically, with a success class that is relatively rare and of particular interest), we can also establish a lower cutoff to better capture those rare successes (at the cost of lumping in more failures as successes). With a lower cutoff, the votes for the *success* class only need attain that lower cutoff level for the entire terminal node to be classified as a *success*. The cutoff therefore determines the proportion of votes needed for determining the terminal node class. See [Evaluating Predictive Performance](evaluating-predictive-performance.ipynb) for further discussion of the use of a cutoff value in classification, for cases where a single class is of interest.\n",
    "\n",
    "In the following sections, we show how trees are constructed and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d848198d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
