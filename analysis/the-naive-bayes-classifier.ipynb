{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7f46d67",
   "metadata": {},
   "source": [
    "# The Naive Bayes Classifier\n",
    "\n",
    "In this chapter, we introduce the naive Bayes classifier, which can be applied to data with categorical predictors. We review the concept of conditional probabilities, then present the complete, or exact, Bayesian classifier. We next see how it is impractical in most cases, and learn how to modify it and use instead the naive Bayes classifier,\n",
    "which is more generally applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff885c6d",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "In this chapter, we will use `pandas` for data handling, `scikit-learn` for naive Bayes models, and `matplotlib` for visualization. We will also make use of the utility functions `dmutils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91faa0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from dmutils import classification_summary, gains_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3654c0ad",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The naive Bayes method (and, indeed, an entire branch of statistics) is named after the Reverend Thomas Bayes (1702–1761). To understand the naive Bayes classifier, we first look at the complete, or exact, Bayesian classifier. The basic principle is simple. For each record to be classified:\n",
    "\n",
    "1. Find all the other records with the same predictor profile (i.e., where the predictor values are the same).\n",
    "2. Determine what classes the records belong to and which class is most prevalent.\n",
    "3. Assign that class to the new record.\n",
    "\n",
    "Alternatively (or in addition), it may be desirable to tweak the method so that it answers the question: \"What is the propensity of belonging to the class of interest?\" instead of \"Which class is the most probable?\". Obtaining class probabilities allows using a sliding cutoff to classify a record as belonging to class $C_i$, even if $C_i$ is not the most probable class for that record. This approach is useful when there is a specific class of interest that we are interested in identifying, and we are willing to \"overidentify\" records as belonging to this class (see [Evaluating Predictive Performance](evaluating-predictive-performance.ipynb) for more details on the use of cutoffs for classification and on asymmetric misclassification costs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca1cd9",
   "metadata": {},
   "source": [
    "### Cutoff Probability Method\n",
    "\n",
    "1. Establish a cutoff probability for the class of interest above which we consider that a record belongs to that class.\n",
    "2. Find all the training records with the same predictor profile as the new record (i.e., where the predictor values are the same).\n",
    "3. Determine the probability that those records belong to the class of interest.\n",
    "4. If that probability is above the cutoff probability, assign the new record to the class of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d094394",
   "metadata": {},
   "source": [
    "### Conditional Probability\n",
    "\n",
    "Both procedures incorporate the concept of *conditional probability*, or the probability of event *A* given that event *B* has occurred, denoted *P(A|B)*. In this case, we will be looking at the probability of the record belonging to class $C_i$ given that its predictor values are $x_1$, $x_2$, ..., $x_p$. In general, for a response with *m* classes $C_1$, $C_2$, ..., $C_m$, and the predictor values $x_1$, $x_2$, ..., $x_p$  we want to compute:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $P(C_i|x_i, ..., x_p)$\n",
    "</p>\n",
    "    \n",
    "To classify a record, we compute its probability of belonging to each of the classes in this way, then classify the record to the class that has the highest probability or use the cutoff probability to decide whether it should be assigned to the class of interest.\n",
    "\n",
    "From this definition, we see that the Bayesian classifier works only with categorical predictors. If we use a set of numerical predictors, then it is highly unlikely that multiple records will have identical values on these numerical predictors. Therefore, numerical predictors must be binned and converted to categorical predictors. *The Bayesian classifier is the only classification or prediction method presented in this book that is especially suited for (and limited to) categorical predictor variables.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89900e6",
   "metadata": {},
   "source": [
    "## Example 1: Predicting Fraudulent Financial Reporting\n",
    "\n",
    "An accounting firm has many large companies as customers. Each customer submits an annual financial report to the firm, which is then audited by the accounting firm. For simplicity, we will designate the outcome of the audit as \"fraudulent\" or \"truthful\", referring to the accounting firm's assessment of the customer's financial report. The\n",
    "accounting firm has a strong incentive to be accurate in identifying fraudulent reports - if it passes a fraudulent report as truthful, it would be in legal trouble.\n",
    "\n",
    "The accounting firm notes that, in addition to all the financial records, it also has information on whether or not the customer has had prior legal trouble (criminal or civil charges of any nature filed against it). This information has not been used in previous audits, but the accounting firm is wondering whether it could be used in the future to identify reports that merit more intensive review. Specifically, it wants to know whether having had prior legal trouble is predictive of fraudulent reporting.\n",
    "\n",
    "In this case, each customer is a record, and the outcome variable of interest, $Y = \\{\\text{fraudulent}, \\text{truthful}\\}$, has two classes into which a company can be classified: $C_1 = \\text{fraudulent}$ and $C_2 = \\text{truthful}$. The predictor variable - \"prior legal trouble\" - has two values: 0 (no prior legal trouble) and 1 (prior legal trouble).\n",
    "\n",
    "The accounting firm has data on 1500 companies that it has investigated in the past. For each company, it has information on whether the financial report was judged fraudulent or truthful and whether the company had prior legal trouble. The data were partitioned into a training set (1000 firms) and a validation set (500 firms). Counts in the training set are shown in the table below:\n",
    "\n",
    "|                   | Prior legal (X=1)| Prior legal (X=0)| Total | \n",
    "|:------------------|:-----------------|:-----------------|:------|\n",
    "| Fraudulent ($C_1$)| 50               | 50               | 100   |\n",
    "| Truthful   ($C_2$)| 180              | 720              | 900   |\n",
    "| Total             | 230              | 770              | 1000  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5025250",
   "metadata": {},
   "source": [
    "## Applying the Full (Exact) Bayesian Classifier\n",
    "\n",
    "Now consider the financial report from a new company, which we wish to classify as either fraudulent or truthful by using these data. To do this, we compute the probabilities, as above, of belonging to each of the two classes.\n",
    "\n",
    "If the new company had had prior legal trouble, the probability of belonging to the fraudulent class would be:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $P(\\text{fraudulent} ∣ \\text{prior legal}) = \\frac{50}{230}$\n",
    "</p>\n",
    "\n",
    "of the 230 companies with prior legal trouble in the training set, 50 had fraudulent financial reports. The probability of belonging to the other class, \"truthful\", is of course, the remainder\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $P(\\text{truthful} ∣ \\text{prior legal}) = \\frac{180}{230}$\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b734d37d",
   "metadata": {},
   "source": [
    "### Using the \"Assign to the Most Probable Class\" Method\n",
    "\n",
    "If a company had prior legal trouble, we assign it to the \"truthful\" class. Similar calculations for the case of no prior legal trouble are computed the same way. In this example, using the rule \"assign to the most probable class\", all records are assigned to the \"truthful\" class. This is the same result as the naive rule of \"assign all records to the majority class\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7142e2fc",
   "metadata": {},
   "source": [
    "### Using Cutoff Probability Method\n",
    "\n",
    "In this example, we are more interested in identifying the fraudulent reports - those are the ones that can land the auditor in jail. We recognize that, in order to identify the fraudulent reports, some truthful reports will be misidentified as fraudulent, and the overall classification accuracy may decline. Our approach is, therefore, to establish a cutoff value for the probability of being fraudulent, and classify all records above that value as fraudulent. The Bayesian formula for the calculation of this probability that a record belongs to class $C_i$ is as follows:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $P(C_i ∣ x_1, ..., x_p) = \\frac{P(x_1, ..., x_p ∣ C_i) P(C_i)}{P(x_1, ..., x_p ∣ C_1) P(C_1) + ... + P(x_1, ..., x_p ∣ C_m) P(C_m)}$\n",
    "</p>\n",
    "\n",
    "In this example (where frauds are rarer), if the cutoff were established at 0.20, we would classify a prior legal trouble record as fraudulent because:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $P(\\text{fraudulent} ∣ \\text{prior legal}) = \\frac{50}{230} = 0.22$\n",
    "</p>\n",
    "        \n",
    "The user can treat this cutoff as a \"slider\" to be adjusted to optimize performance, like other parameters in any classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d5ed23",
   "metadata": {},
   "source": [
    "### Practical Difficulty with the Complete (Exact) Bayes Procedure\n",
    "\n",
    "The approach outlined above amounts to finding all the records in the sample that are exactly like the new record to be classified in the sense that all the predictor values are all identical. This was easy in the small example presented above, where there was just one predictor.\n",
    "\n",
    "When the number of predictors gets larger (even to a modest number like 20), many of the records to be classified will be without exact matches. This can be understood in the context of a model to predict voting on the basis of demographic variables. Even a sizable sample may not contain even a single match for a new record who is a male Hispanic with high income from the US Midwest who voted in the last election, did not vote in the prior election, has three daughters and one son, and is divorced. And this is just eight variables, a small number for most data mining exercises. The addition of just a single new variable with five equally frequent categories reduces the probability of a match by a factor of 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5b9ceb",
   "metadata": {},
   "source": [
    "### Solution: Naive Bayes\n",
    "\n",
    "In the naive Bayes solution, we no longer restrict the probability calculation to those records that match the record to be classified. Instead we use the entire dataset.\n",
    "\n",
    "Returning to our original basic classification procedure outlined at the beginning of the chapter, recall that the procedure for classifying a new record was:\n",
    "\n",
    "1. Find all the other records with the same predictor profile (i.e., where the predictor values are the same).\n",
    "2. Determine what classes the records belong to and which class is most prevalent.\n",
    "3. Assign that class to the new record.\n",
    "\n",
    "*The naive Bayes modification (for the basic classification procedure) is as follows:*\n",
    "\n",
    "1. For class C 1 , estimate the individual conditional probabilities for each predictor $P(x_j | C_1)$ - these are the probabilities that the predictor value in the record to be classified occurs in class $C_1$. For example, for $X_1$ this probability is estimated by the proportion of $x_1$ values among the $C_1$ records in the training set.\n",
    "2. Multiply these probabilities by each other, then by the proportion of records belonging to class $C_1$.\n",
    "3. Repeat Steps 1 and 2 for all the classes.\n",
    "4. Estimate a probability for class $C_i$ by taking the value calculated in Step 2 for class $C_i$ and dividing it by the sum of such values for all classes.\n",
    "5. Assign the record to the class with the highest probability for this set of predictor values.\n",
    "\n",
    "The above steps lead to the naive Bayes formula for calculating the probability that a record with a given set of predictor values $x_1$, ..., $x_p$ belongs to class $C_1$ among $m$ classes. The formula can be written as follows:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $P_{nb}(C_1 ∣ x_1, ..., x_p) = \\frac{P(C_1)[P(x_1 ∣ C_1) P(x_2 ∣ C_1)...P(x_p ∣ C_1)]}{P(C_1)[P(x_1 ∣ C_1) P(x_2 ∣ C_1)...P(x_p ∣ C_1)] + ... + P(C_m)[P(x_1 ∣ C_m) P(x_2 ∣ C_m)...P(x_p ∣ C_m)]}$\n",
    "</p>\n",
    "\n",
    "This is a somewhat formidable formula; see Example 2 for a simpler numerical version. Note that all the needed quantities can be obtained from pivot tables of *Y* vs. each of the categorical predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8d5523",
   "metadata": {},
   "source": [
    "### The Naive Bayes Assumption of Conditional Independence\n",
    "\n",
    "In probability terms, we have made a simplifying assumption that the exact *conditional probability* of seeing a record with predictor profile $x_1$, $x_2$, ..., $x_p$ within a certain class, \n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $P(x_1, x_2, ..., x_p | C_i)$,\n",
    "</p>\n",
    "\n",
    "is well approximated by the product of the individual conditional probabilities\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $P(x_1 | C_i ) × P(x_2 | C_i) × ... × P(x_p | C_i)$.\n",
    "</p>\n",
    "\n",
    "These two quantities are identical when the predictors are independent within each class.\n",
    "\n",
    "For example, suppose that \"lost money last year\" is an additional variable in the accounting fraud example. The simplifying assumption we make with naive Bayes is that, within a given class, we no longer need to look for the records characterized both by \"prior legal trouble\" and \"lost money last year\". Rather, assuming that the two are\n",
    "independent, we can simply multiply the probability of \"prior legal trouble\" by the probability of \"lost money last year\". Of course, complete independence is unlikely in practice, where some correlation between predictors is expected.\n",
    "\n",
    "In practice, despite the assumption violation, the procedure works quite well - primarily because what is usually needed is not a propensity for each record that is accurate in absolute terms but just a reasonably accurate *rank ordering* of propensities. Even when the assumption is violated, the rank ordering of the records' propensities is typically preserved.\n",
    "\n",
    "Note that if all we are interested in is a rank ordering, and the denominator remains the same for all classes, it is sufficient to concentrate only on the numerator. The disadvantage of this approach is that the probability values it yields (the propensities), while ordered correctly, are not on the same scale as the exact values that the user\n",
    "would anticipate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6398ecb6",
   "metadata": {},
   "source": [
    "### Using the Cutoff Probability Method\n",
    "\n",
    "The above procedure is for the basic case where we seek maximum classification accuracy for all classes. In the case of the *relatively rare class of special interest*, the procedure is:\n",
    "\n",
    "1. Establish a cutoff probability for the class of interest above which we consider that a record belongs to that class.\n",
    "2. For the class of interest, compute the probability that each individual predictor value in the record to be classified occurs in the training data.\n",
    "3. Multiply these probabilities times each other, then times the proportion of records belonging to the class of interest.\n",
    "4. Estimate the probability for the class of interest by taking the value calculated in Step 3 for the class of interest and dividing it by the sum of the similar values for all classes.\n",
    "5. If this value falls above the cutoff, assign the new record to the class of interest, otherwise not.\n",
    "6. Adjust the cutoff value as needed, as a parameter of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e92c1e",
   "metadata": {},
   "source": [
    "## Example 2: Predicting Fraudulent Financial Reports, Two Predictors\n",
    "\n",
    "Let us expand the financial reports example to two predictors, and, using a small subset of data, compare the complete (exact) Bayes calculations to the naive Bayes calculations.\n",
    "\n",
    "Consider the 10 customers of the accounting firm listed below. For each customer, we have information on whether it had prior legal trouble, whether it is a small or large company, and whether the financial report was found to be fraudulent or truthful. Using this information, we will calculate the conditional probability of fraud, given each of the four possible combinations $\\{\\text{y}, \\text{small}\\}$, $\\{\\text{y}, \\text{large}\\}$, $\\{\\text{n}, \\text{small}\\}$, $\\{\\text{n}, \\text{large}\\}$.\n",
    "\n",
    "| Company           | Prior legal trouble| Company Size     | Status     | \n",
    "|:------------------|:-------------------|:-----------------|:-----------|\n",
    "| 1                 | Yes                | Small            | Truthful   |\n",
    "| 2                 | No                 | Small            | Truthful   |\n",
    "| 3                 | No                 | Large            | Truthful   |\n",
    "| 4                 | No                 | Large            | Truthful   |\n",
    "| 5                 | No                 | Small            | Truthful   |\n",
    "| 6                 | No                 | Small            | Truthful   |\n",
    "| 7                 | Yes                | Small            | Fraudulent |\n",
    "| 8                 | Yes                | Large            | Fraudulent |\n",
    "| 9                 | No                 | Large            | Fraudulent |\n",
    "| 10                | Yes                | Large            | Fraudulent |\n",
    "\n",
    "**Complete (Exact) Bayes Calculations**\n",
    "\n",
    "The probabilities are computed as:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    <br>$P(\\text{fraudulent} ∣ \\text{Prior legal = y}, \\text{Size = small}) = \\frac{1}{2} = 0.5$\n",
    "    <br>$P(\\text{fraudulent} ∣ \\text{Prior legal = y}, \\text{Size = large}) = \\frac{2}{2} = 1$\n",
    "    <br>$P(\\text{fraudulent} ∣ \\text{Prior legal = n}, \\text{Size = small}) = \\frac{0}{3} = 0$\n",
    "    <br>$P(\\text{fraudulent} ∣ \\text{Prior legal = n}, \\text{Size = large}) = \\frac{1}{3} = 0.33$\n",
    "</p>\n",
    "\n",
    "**Naive Bayes Calculations**\n",
    "\n",
    "Now we compute the naive Bayes probabilities. For the conditional probability of fraudulent behaviors given $\\{\\text{prior legal = y}, \\text{Size = small}\\}$ the numerator is a multiplication of the proportion of $\\{\\text{Prior legal = y}\\}$ instances among the fraudulent companies, times the proportion of $\\{\\text{Size = small}\\}$ instances among the fraudulent companies, times the proportion of fraudulent companies:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $\\frac{3}{4} \\times \\frac{1}{4} \\times \\frac{4}{10} = 0.075$\n",
    "</p>\n",
    "\n",
    "To get the actual probabilities, we must also compute the numerator for the conditional probability of truthful behaviors given $\\{\\text{Prior Legal = y}, \\text{Size = small}\\}$:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $\\frac{1}{6} \\times \\frac{4}{6} \\times \\frac{6}{10} = 0.067$\n",
    "</p>\n",
    "\n",
    "The denominator is then the sum of these two conditional probabilities:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $ 0.075 + 0.067 = 0.14$\n",
    "</p>\n",
    "\n",
    "The conditional probability of fraudulent behaviors given $\\{\\text{Prior legal = y}, \\text{Size = small}\\}$ is therefore:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $\\frac{0.075}{0.14} = 0.53$\n",
    "</p>\n",
    "\n",
    "In a similar fashion, we compute all four conditional probabilities:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    <br>$P_{nb}(\\text{fraudulent} ∣ \\text{Prior legal = y}, \\text{Size = small}) = \\frac{(\\frac{3}{4})(\\frac{1}{4})(\\frac{6}{10})}{(\\frac{3}{4})(\\frac{1}{4})(\\frac{3}{4}) + (\\frac{1}{6})(\\frac{4}{6})(\\frac{6}{10})} = 0.53$\n",
    "    <br>$P_{nb}(\\text{fraudulent} ∣ \\text{Prior legal = y}, \\text{Size = large}) = 0.87$\n",
    "    <br>$P_{nb}(\\text{fraudulent} ∣ \\text{Prior legal = n}, \\text{Size = small}) = 0.07$\n",
    "    <br>$P_{nb}(\\text{fraudulent} ∣ \\text{Prior legal = n}, \\text{Size = large}) = 0.31$\n",
    "</p>\n",
    "\n",
    "Note how close these naive Bayes probabilities are to the exact Bayes probabilities. Although they are not equal, both would lead to exactly the same classification for a cutoff of 0.5 (and many other values). It is often the case that the rank ordering of probabilities is even closer to the exact Bayes method than the probabilities themselves, and for classification purposes it is the rank orderings that matter.\n",
    "\n",
    "We now consider a larger numerical example, where information on flights is used to predict flight delays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4766b636",
   "metadata": {},
   "source": [
    "## Example 3: Predicting Delayed Flights\n",
    "\n",
    "Predicting flight delays can be useful to a variety of organizations: airport authorities, airlines, and aviation authorities. At times, joint task forces have been formed to address the problem. If such an organization were to provide ongoing real-time assistance with flight delays, it would benefit from some advance notice about flights that are likely to be delayed.\n",
    "\n",
    "In this simplified illustration, we look at five predictors (see below). The outcome of interest is whether or not the flight is delayed (*delayed* here means arrived more than 15 minutes late). Our data consist of all flights from the Washington, DC area into the New York City area during January 2004. A record is a particular flight. The percentage of delayed flights among these 2201 flights is 19.5%. The data were obtained from the Bureau of Transportation Statistics (available at www.transtats.bts.gov). The goal is to accurately predict whether or not a new flight (not in this dataset), will be delayed. The outcome variable is whether the flight was delayed or not (1 = delayed and 0 = on time). In addition, information is collected on the predictors listed next:\n",
    "\n",
    "    Day of week: Coded as 1 = Monday, 2 = Tuesday, ..., 7 = Sunday\n",
    "    Sch. dep. time: Broken down into 18 intervals between 6:00 AM and 10:00 PM \n",
    "    Origin: Three airport codes: DCA (Reagan National), IAD (Dulles), BWI (Baltimore–Washington Int'l)\n",
    "    Destination: Three airport codes: JFK (Kennedy), LGA (LaGuardia), EWR (Newark)\n",
    "    Carrier: Eight airline codes: CO (Continental), DH (Atlantic Coast), DL (Delta),\n",
    "             MQ (American Eagle), OH (Comair), RU (Continental Express), UA (United), and US (USAirways)\n",
    "\n",
    "After converting all predictors to categorical and creating dummies, the data were partitioned into training (60%) and validation (40%) sets, and then a naive Bayes classifier was applied to the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6762a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_df = pd.read_csv(\"../datasets/FlightDelays.csv\")\n",
    "\n",
    "# convert to categorical\n",
    "delays_df.DAY_WEEK = delays_df.DAY_WEEK.astype(\"category\")\n",
    "delays_df[\"Flight Status\"] = delays_df[\"Flight Status\"].astype(\"category\")\n",
    "\n",
    "# create hourly bins departure\n",
    "delays_df.CRS_DEP_TIME = [round(t/100) for t in delays_df.CRS_DEP_TIME]\n",
    "delays_df.CRS_DEP_TIME = delays_df.CRS_DEP_TIME.astype(\"category\")\n",
    "\n",
    "predictors = [\"DAY_WEEK\", \"CRS_DEP_TIME\", \"ORIGIN\", \"DEST\", \"CARRIER\"]\n",
    "outcome = \"Flight Status\"\n",
    "\n",
    "X = pd.get_dummies(delays_df[predictors])\n",
    "y = delays_df[\"Flight Status\"].astype(\"category\")\n",
    "classes = list(y.cat.categories)\n",
    "\n",
    "# split into training and validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.40, random_state=1)\n",
    "\n",
    "# run naive Bayes\n",
    "delays_nb = MultinomialNB(alpha=0.01)\n",
    "delays_nb.fit(X_train, y_train)\n",
    "\n",
    "# predict probabilities\n",
    "pred_proba_train = delays_nb.predict_proba(X_train)\n",
    "pred_proba_valid = delays_nb.predict_proba(X_valid)\n",
    "\n",
    "# predict class membership\n",
    "y_train_pred = delays_nb.predict(X_train)\n",
    "y_valid_pred = delays_nb.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d285befd",
   "metadata": {},
   "source": [
    "Before using the output, let's see how the algorithm works. We start by generating pivot tables for the outcome vs. each of the five predictors using the training set, in order to obtain conditional probabilities (table below). Note that in this example, there are no  predictor values that were not represented in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ebcbb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ontime     0.8023\n",
      "delayed    0.1977\n",
      "Name: Flight Status, dtype: float64\n",
      "\n",
      "DAY_WEEK            1       2       3       4       5      6       7\n",
      "Flight Status                                                       \n",
      "delayed        0.1916  0.1494  0.1149  0.1264  0.1877  0.069  0.1609\n",
      "ontime         0.1246  0.1416  0.1445  0.1794  0.1690  0.136  0.1048\n",
      "\n",
      "CRS_DEP_TIME        6       7       8       9      10      11      12      13  \\\n",
      "Flight Status                                                                   \n",
      "delayed        0.0345  0.0536  0.0651  0.0192  0.0307  0.0115  0.0498  0.0460   \n",
      "ontime         0.0623  0.0633  0.0850  0.0567  0.0519  0.0340  0.0661  0.0746   \n",
      "\n",
      "CRS_DEP_TIME       14      15      16      17      18      19      20      21  \n",
      "Flight Status                                                                  \n",
      "delayed        0.0383  0.2031  0.0728  0.1533  0.0192  0.0996  0.0153  0.0881  \n",
      "ontime         0.0576  0.1171  0.0774  0.1001  0.0349  0.0397  0.0264  0.0529  \n",
      "\n",
      "ORIGIN            BWI     DCA     IAD\n",
      "Flight Status                        \n",
      "delayed        0.0805  0.5211  0.3985\n",
      "ontime         0.0604  0.6478  0.2918\n",
      "\n",
      "DEST              EWR     JFK     LGA\n",
      "Flight Status                        \n",
      "delayed        0.3793  0.1992  0.4215\n",
      "ontime         0.2663  0.1558  0.5779\n",
      "\n",
      "CARRIER            CO      DH      DL      MQ      OH      RU      UA      US\n",
      "Flight Status                                                                \n",
      "delayed        0.0575  0.3142  0.0958  0.2222  0.0077  0.2184  0.0153  0.0690\n",
      "ontime         0.0349  0.2295  0.2040  0.1171  0.0104  0.1690  0.0170  0.2181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split the original data frame into a train and test using the same\n",
    "# random_state\n",
    "train_df, valid_df = train_test_split(delays_df, test_size=0.40, random_state=1)\n",
    "\n",
    "pd.set_option(\"precision\", 4)\n",
    "# probability of flight status\n",
    "print(train_df[\"Flight Status\"].value_counts() / len(train_df))\n",
    "print()\n",
    "\n",
    "for predictor in predictors:\n",
    "    # construct the frequency table\n",
    "    df = train_df[[\"Flight Status\", predictor]]\n",
    "    freq_table = df.pivot_table(index=\"Flight Status\", columns=predictor, aggfunc=len)\n",
    "\n",
    "    # divide each value by the sum of the row to get conditional probabilities\n",
    "    prop_table = freq_table.apply(lambda x: x/sum(x), axis=1)\n",
    "    print(prop_table)\n",
    "    print()\n",
    "\n",
    "pd.reset_option(\"precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a6543c",
   "metadata": {},
   "source": [
    "To classify a new flight, we compute the probability that it will be delayed and the probability that it will be on time. Recall that since both probabilities will have the same denominator, we can just compare the numerators. Each numerator is computed by multiplying all the conditional probabilities of the relevant predictor values and, finally, multiplying by the proportion of that class (in this case $\\hat{P}$(delayed) = 0.2). Let us use an example: to classify a Delta flight from DCA to LGA departing between 10:00 AM and 11:00 AM on a Sunday, we first compute the numerators using the values from the pivot tables:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $\\hat{P}(\\text{delayed} ∣ \\text{Carrier=DL}, \\text{Day_Week}=7, \\text{Dep_Time}=10, \\text{Dest=LGA}, \\text{Origin=DCA}) = \\propto (0.2)(0.0958)(0.1609)(0.4215)(0.0307)(0.5211) = 0.000021$\n",
    "    <br>$\\hat{P}(\\text{ontime} ∣ \\text{Carrier=DL}, \\text{Day_Week}=7, \\text{Dep_Time}=10, \\text{Dest=LGA}, \\text{Origin=DCA}) = \\propto (0.8)(0.2040)(0.1048)(0.05779)(0.0519)(0.6478) = 0.000033$\n",
    "</p>\n",
    "\n",
    "The symbol $\\propto$ means \"is proportional to\", reflecting the fact that this calculation deals only with the numerator in the naive Bayes formula. Comparing the numerators, it is therefore, more likely that the flight will be on time. Note that a record with such a combination of predictor values does not exist in the training set, and therefore we use the naive Bayes rather than the exact Bayes. To compute the actual probability, we divide each of the numerators by their sum:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $\\hat{P}(\\text{delayed} ∣ \\text{Carrier=DL}, \\text{Day_Week}=7, \\text{Dep_Time}=10, \\text{Dest=LGA}, \\text{Origin=DCA}) = \\frac{0.000021}{0.000021+0.000033} = 0.058$,\n",
    "    <br>$\\hat{P}(\\text{ontime} ∣ \\text{Carrier=DL}, \\text{Day_Week}=7, \\text{Dep_Time}=10, \\text{Dest=LGA}, \\text{Origin=DCA}) = \\frac{0.000033}{0.000021+0.000033} = 0.942$\n",
    "</p>\n",
    "\n",
    "Of course, we rely on software to compute these probabilities for any records of interest (in the training set, the validation set, or for scoring new data). The following table shows the predicted probability and class for the example flight, which coincide with our manual calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5babd1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>ontime</td>\n",
       "      <td>ontime</td>\n",
       "      <td>0.057989</td>\n",
       "      <td>0.942011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual predicted         0         1\n",
       "1225  ontime    ontime  0.057989  0.942011"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classify a specific flight by searching in the dataset\n",
    "# for a flight with the same predictor values\n",
    "df = pd.concat([pd.DataFrame({\"actual\": y_valid, \"predicted\": y_valid_pred}),\n",
    "                pd.DataFrame(pred_proba_valid, index=y_valid.index)], axis=1)\n",
    "\n",
    "mask = ((X_valid.CARRIER_DL == 1) & (X_valid.DAY_WEEK_7 == 1) &\n",
    "        (X_valid.CRS_DEP_TIME_10 == 1) & (X_valid.DEST_LGA == 1) &\n",
    "        (X_valid.ORIGIN_DCA == 1))\n",
    "df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aea5cc0",
   "metadata": {},
   "source": [
    "Finally, to evaluate the performance of the naive Bayes classifier for our data, we can use the confusion matrix, gains and lift charts, and all the measures that were described in [Evaluating Predictive Performance](evaluating-predictive-performance.ipynb). For our example, the confusion matrices for the training and validation sets are shown below. We see that the overall accuracy level is around 80% for both the training and validation data. In comparison, a naive rule that would classify all 880 flights in the validation set as \"on time\" would have missed the 172 delayed flights, also resulting in a 80% accuracy. Thus, by a simple accuracy measure, the naive Bayes model does no better than the naive rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d7e16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Accuracy 0.7955)\n",
      "\n",
      "        Prediction\n",
      " Actual delayed  ontime\n",
      "delayed      52     209\n",
      " ontime      61     998\n",
      "\n",
      "Confusion Matrix (Accuracy 0.7821)\n",
      "\n",
      "        Prediction\n",
      " Actual delayed  ontime\n",
      "delayed      26     141\n",
      " ontime      51     663\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "classification_summary(y_train, y_train_pred, class_names=classes)\n",
    "print()\n",
    "\n",
    "# validation\n",
    "classification_summary(y_valid, y_valid_pred, class_names=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec1a7dd",
   "metadata": {},
   "source": [
    "However, examining the gains and lift charts shows the strength of the naive Bayes in capturing the delayed flights effectively, when the goal is ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a488cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"actual\": 1 - y_valid.cat.codes, \"prob\": pred_proba_valid[:, 0]})\n",
    "df = df.sort_values(by=[\"prob\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "gains_chart(df.actual, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b872c077",
   "metadata": {},
   "source": [
    "## Advantages and Shortcomings of the Naive Bayes Classifier\n",
    "\n",
    "The naive Bayes classifier's beauty is in its simplicity, computational efficiency, good classification performance, and ability to handle categorical variables directly. In fact, it often outperforms more sophisticated classifiers even when the underlying assumption of independent predictors is far from true. This advantage is especially pronounced when the number of predictors is very large.\n",
    "\n",
    "Three main issues should be kept in mind, however. First, the naive Bayes classifier requires a very large number of records to obtain good results.\n",
    "\n",
    "Second, where a predictor category is not present in the training data, naive Bayes assumes that a new record with that category of the predictor has zero probability. This can be a problem if this rare predictor value is important. One example is the binary predictor *Weather* in the flights delay dataset, which we did not use for analysis, and which denotes bad weather. Consider another example, where the outcome variable is *bought high-value life insurance* and a predictor category is *owns yacht*. If the training data have no records with *owns yacht* = 1, for any new records where *owns yacht* = 1, naive Bayes will assign a probability of 0 to the outcome variable *bought high-value life insurance*. With no training records with *owns yacht* = 1, of course, no data mining technique will be able to incorporate this potentially important variable into the classification model - it will\n",
    "be ignored. With naive Bayes, however, the absence of this predictor actively \"outvotes\" any other information in the record to assign a 0 to the outcome value (when, in this case, it has a relatively good chance of being a 1). The presence of a large training set (and judicious binning of continuous predictors, if required) helps mitigate this effect. A popular solution in such cases is to replace zero probabilities with non-zero values using a method called *smoothing* (e.g., Laplace smoothing can be applied by using argument `alpha` > 0 in function `MultinomialNB`; by default scikit-learn already uses a smoothing parameter of 1).\n",
    "\n",
    "Finally, good performance is obtained when the goal is *classification* or *ranking* of records according to their probability of belonging to a certain class. However, when the goal is to estimate the probability of *class membership* (*propensity*), this method provides very biased results. For this reason, the naive Bayes method is rarely used in credit scoring (Larsen, 2005).\n",
    "\n",
    "**SPAM Filtering**\n",
    "\n",
    "Filtering spam in e-mail has long been a widely familiar application of data mining. Spam filtering, which is based in large part on natural language vocabulary, is a natural fit for a naive Bayesian classifier, which uses exclusively categorical variables. Most spam filters are based on this method, which works as follows:\n",
    "\n",
    "1. Humans review a large number of e-mails, classify them as \"spam\" or \"not spam\", and from these select an equal (also large) number of spam e-mails and non-spam e-mails. This is the training data.\n",
    "\n",
    "2. These e-mails will contain thousands of words; for each word, compute the frequency with which it occurs in the spam dataset, and the frequency with which it occurs in the non-spam dataset. Convert these frequencies into estimated probabilities (i.e., if the word \"free\" occurs in 500 out of 1000 spam e-mails, and only 100 out of 1000 non-spam e-mails, the probability that a spam e-mail will contain the word \"free\" is 0.5, and the probability that a non-spam e-mail will contain the word \"free\" is 0.1).\n",
    "\n",
    "3. If the only word in a new message that needs to be classified as spam or not spam is \"free\", we would classify the message as spam, since the Bayesian posterior probability is 0.5/(0.5 + 0.1) or 5/6 that, given the appearance of \"free\", the message is spam.\n",
    "\n",
    "4. Of course, we will have many more words to consider. For each such word, the probabilities described in Step 2 are calculated, and multiplied together, and formula of Naive Bayes is applied to determine the naive Bayes probability of belonging to the classes. In the simple version, class membership (spam or not spam) is determined by the higher probability.\n",
    "\n",
    "5. In a more flexible interpretation, the ratio between the \"spam\" and \"not spam\" probabilities is treated as a score for which the operator can establish (and change) a cutoff threshold - anything above that level is classified as spam.\n",
    "\n",
    "6. Users have the option of building a personalized training database by classifying incoming messages as spam or not spam, and adding them to the training database. One person's spam may be another person's substance.\n",
    "\n",
    "It is clear that, even with the \"Naive\" simplification, this is an enormous computational burden. Spam filters now typically operate at two levels - at servers (intercepting some spam that never makes it to your computer) and on individual computers (where you have the option of reviewing it). Spammers have also found ways to \"poison\" the vocabulary-based Bayesian approach, by including sequences of randomly selected irrelevant words. Since these words are randomly selected, they are unlikely to be systematically more prevalent in spam than in non-spam, and they dilute the effect of key spam terms such as \"Viagra\" and \"free\". For this reason, sophisticated spam classifiers also include variables based on elements other than vocabulary, such as the number of links in the message, the vocabulary in the subject line, determination of whether the \"From\": e-mail address is the real originator (anti-spoofing), use of HTML and images, and origination at a dynamic or static IP address (the latter are more expensive and cannot be set up quickly)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
