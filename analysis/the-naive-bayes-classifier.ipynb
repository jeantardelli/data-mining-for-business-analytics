{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7f46d67",
   "metadata": {},
   "source": [
    "# The Naive Bayes Classifier\n",
    "\n",
    "In this chapter, we introduce the naive Bayes classifier, which can be applied to data with categorical predictors. We review the concept of conditional probabilities, then present the complete, or exact, Bayesian classifier. We next see how it is impractical in most cases, and learn how to modify it and use instead the naive Bayes classifier,\n",
    "which is more generally applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff885c6d",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "In this chapter, we will use `pandas` for data handling, `scikit-learn` for naive Bayes models, and `matplotlib` for visualization. We will also make use of the utility functions `dmutils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91faa0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from dmutils import classification_summary, gains_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3654c0ad",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The naive Bayes method (and, indeed, an entire branch of statistics) is named after the Reverend Thomas Bayes (1702–1761). To understand the naive Bayes classifier, we first look at the complete, or exact, Bayesian classifier. The basic principle is simple. For each record to be classified:\n",
    "\n",
    "1. Find all the other records with the same predictor profile (i.e., where the predictor values are the same).\n",
    "2. Determine what classes the records belong to and which class is most prevalent.\n",
    "3. Assign that class to the new record.\n",
    "\n",
    "Alternatively (or in addition), it may be desirable to tweak the method so that it answers the question: \"What is the propensity of belonging to the class of interest?\" instead of \"Which class is the most probable?\". Obtaining class probabilities allows using a sliding cutoff to classify a record as belonging to class $C_i$, even if $C_i$ is not the most probable class for that record. This approach is useful when there is a specific class of interest that we are interested in identifying, and we are willing to \"overidentify\" records as belonging to this class (see [Evaluating Predictive Performance](evaluating-predictive-performance.ipynb) for more details on the use of cutoffs for classification and on asymmetric misclassification costs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca1cd9",
   "metadata": {},
   "source": [
    "### Cutoff Probability Method\n",
    "\n",
    "1. Establish a cutoff probability for the class of interest above which we consider that a record belongs to that class.\n",
    "2. Find all the training records with the same predictor profile as the new record (i.e., where the predictor values are the same).\n",
    "3. Determine the probability that those records belong to the class of interest.\n",
    "4. If that probability is above the cutoff probability, assign the new record to the class of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d094394",
   "metadata": {},
   "source": [
    "### Conditional Probability\n",
    "\n",
    "Both procedures incorporate the concept of *conditional probability*, or the probability of event *A* given that event *B* has occurred, denoted *P(A|B)*. In this case, we will be looking at the probability of the record belonging to class $C_i$ given that its predictor values are $x_1$, $x_2$, ..., $x_p$. In general, for a response with *m* classes $C_1$, $C_2$, ..., $C_m$, and the predictor values $x_1$, $x_2$, ..., $x_p$  we want to compute:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $P(C_i|x_i, ..., x_p)$\n",
    "</p>\n",
    "    \n",
    "To classify a record, we compute its probability of belonging to each of the classes in this way, then classify the record to the class that has the highest probability or use the cutoff probability to decide whether it should be assigned to the class of interest.\n",
    "\n",
    "From this definition, we see that the Bayesian classifier works only with categorical predictors. If we use a set of numerical predictors, then it is highly unlikely that multiple records will have identical values on these numerical predictors. Therefore, numerical predictors must be binned and converted to categorical predictors. *The Bayesian classifier is the only classification or prediction method presented in this book that is especially suited for (and limited to) categorical predictor variables.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89900e6",
   "metadata": {},
   "source": [
    "## Example 1: Predicting Fraudulent Financial Reporting\n",
    "\n",
    "An accounting firm has many large companies as customers. Each customer submits an annual financial report to the firm, which is then audited by the accounting firm. For simplicity, we will designate the outcome of the audit as \"fraudulent\" or \"truthful\", referring to the accounting firm's assessment of the customer's financial report. The\n",
    "accounting firm has a strong incentive to be accurate in identifying fraudulent reports - if it passes a fraudulent report as truthful, it would be in legal trouble.\n",
    "\n",
    "The accounting firm notes that, in addition to all the financial records, it also has information on whether or not the customer has had prior legal trouble (criminal or civil charges of any nature filed against it). This information has not been used in previous audits, but the accounting firm is wondering whether it could be used in the future to identify reports that merit more intensive review. Specifically, it wants to know whether having had prior legal trouble is predictive of fraudulent reporting.\n",
    "\n",
    "In this case, each customer is a record, and the outcome variable of interest, $Y = \\{\\text{fraudulent}, \\text{truthful}\\}$, has two classes into which a company can be classified: $C_1 = \\text{fraudulent}$ and $C_2 = \\text{truthful}$. The predictor variable - \"prior legal trouble\" - has two values: 0 (no prior legal trouble) and 1 (prior legal trouble).\n",
    "\n",
    "The accounting firm has data on 1500 companies that it has investigated in the past. For each company, it has information on whether the financial report was judged fraudulent or truthful and whether the company had prior legal trouble. The data were partitioned into a training set (1000 firms) and a validation set (500 firms). Counts in the training set are shown in the table below:\n",
    "\n",
    "|                   | Prior legal (X=1)| Prior legal (X=0)| Total | \n",
    "|:------------------|:-----------------|:-----------------|:------|\n",
    "| Fraudulent ($C_1$)| 50               | 50               | 100   |\n",
    "| Truthful   ($C_2$)| 180              | 720              | 900   |\n",
    "| Total             | 230              | 770              | 1000  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5025250",
   "metadata": {},
   "source": [
    "## Applying the Full (Exact) Bayesian Classifier\n",
    "\n",
    "Now consider the financial report from a new company, which we wish to classify as either fraudulent or truthful by using these data. To do this, we compute the probabilities, as above, of belonging to each of the two classes.\n",
    "\n",
    "If the new company had had prior legal trouble, the probability of belonging to the fraudulent class would be:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $P(\\text{fraudulent} ∣ \\text{prior legal}) = \\frac{50}{230}$\n",
    "</p>\n",
    "\n",
    "of the 230 companies with prior legal trouble in the training set, 50 had fraudulent financial reports. The probability of belonging to the other class, \"truthful\", is of course, the remainder\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $P(\\text{truthful} ∣ \\text{prior legal}) = \\frac{180}{230}$\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b734d37d",
   "metadata": {},
   "source": [
    "### Using the \"Assign to the Most Probable Class\" Method\n",
    "\n",
    "If a company had prior legal trouble, we assign it to the \"truthful\" class. Similar calculations for the case of no prior legal trouble are computed the same way. In this example, using the rule \"assign to the most probable class\", all records are assigned to the \"truthful\" class. This is the same result as the naive rule of \"assign all records to the majority class\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7142e2fc",
   "metadata": {},
   "source": [
    "### Using Cutoff Probability Method\n",
    "\n",
    "In this example, we are more interested in identifying the fraudulent reports - those are the ones that can land the auditor in jail. We recognize that, in order to identify the fraudulent reports, some truthful reports will be misidentified as fraudulent, and the overall classification accuracy may decline. Our approach is, therefore, to establish a cutoff value for the probability of being fraudulent, and classify all records above that value as fraudulent. The Bayesian formula for the calculation of this probability that a record belongs to class $C_i$ is as follows:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $P(C_i ∣ x_1, ..., x_p) = \\frac{P(x_1, ..., x_p ∣ C_i) P(C_i)}{P(x_1, ..., x_p ∣ C_1) P(C_1) + ... + P(x_1, ..., x_p ∣ C_m) P(C_m)}$\n",
    "</p>\n",
    "\n",
    "In this example (where frauds are rarer), if the cutoff were established at 0.20, we would classify a prior legal trouble record as fraudulent because:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $P(\\text{fraudulent} ∣ \\text{prior legal}) = \\frac{50}{230} = 0.22$\n",
    "</p>\n",
    "        \n",
    "The user can treat this cutoff as a \"slider\" to be adjusted to optimize performance, like other parameters in any classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d5ed23",
   "metadata": {},
   "source": [
    "### Practical Difficulty with the Complete (Exact) Bayes Procedure\n",
    "\n",
    "The approach outlined above amounts to finding all the records in the sample that are exactly like the new record to be classified in the sense that all the predictor values are all identical. This was easy in the small example presented above, where there was just one predictor.\n",
    "\n",
    "When the number of predictors gets larger (even to a modest number like 20), many of the records to be classified will be without exact matches. This can be understood in the context of a model to predict voting on the basis of demographic variables. Even a sizable sample may not contain even a single match for a new record who is a male Hispanic with high income from the US Midwest who voted in the last election, did not vote in the prior election, has three daughters and one son, and is divorced. And this is just eight variables, a small number for most data mining exercises. The addition of just a single new variable with five equally frequent categories reduces the probability of a match by a factor of 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5b9ceb",
   "metadata": {},
   "source": [
    "### Solution: Naive Bayes\n",
    "\n",
    "In the naive Bayes solution, we no longer restrict the probability calculation to those records that match the record to be classified. Instead we use the entire dataset.\n",
    "\n",
    "Returning to our original basic classification procedure outlined at the beginning of the chapter, recall that the procedure for classifying a new record was:\n",
    "\n",
    "1. Find all the other records with the same predictor profile (i.e., where the predictor values are the same).\n",
    "2. Determine what classes the records belong to and which class is most prevalent.\n",
    "3. Assign that class to the new record.\n",
    "\n",
    "*The naive Bayes modification (for the basic classification procedure) is as follows:*\n",
    "\n",
    "1. For class C 1 , estimate the individual conditional probabilities for each predictor $P(x_j | C_1)$ - these are the probabilities that the predictor value in the record to be classified occurs in class $C_1$. For example, for $X_1$ this probability is estimated by the proportion of $x_1$ values among the $C_1$ records in the training set.\n",
    "2. Multiply these probabilities by each other, then by the proportion of records belonging to class $C_1$.\n",
    "3. Repeat Steps 1 and 2 for all the classes.\n",
    "4. Estimate a probability for class $C_i$ by taking the value calculated in Step 2 for class $C_i$ and dividing it by the sum of such values for all classes.\n",
    "5. Assign the record to the class with the highest probability for this set of predictor values.\n",
    "\n",
    "The above steps lead to the naive Bayes formula for calculating the probability that a record with a given set of predictor values $x_1$, ..., $x_p$ belongs to class $C_1$ among $m$ classes. The formula can be written as follows:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $P_{nb}(C_1 ∣ x_1, ..., x_p) = \\frac{P(C_1)[P(x_1 ∣ C_1) P(x_2 ∣ C_1)...P(x_p ∣ C_1)]}{P(C_1)[P(x_1 ∣ C_1) P(x_2 ∣ C_1)...P(x_p ∣ C_1)] + ... + P(C_m)[P(x_1 ∣ C_m) P(x_2 ∣ C_m)...P(x_p ∣ C_m)]}$\n",
    "</p>\n",
    "\n",
    "This is a somewhat formidable formula; see Example 2 for a simpler numerical version. Note that all the needed quantities can be obtained from pivot tables of *Y* vs. each of the categorical predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8d5523",
   "metadata": {},
   "source": [
    "### The Naive Bayes Assumption of Conditional Independence\n",
    "\n",
    "In probability terms, we have made a simplifying assumption that the exact *conditional probability* of seeing a record with predictor profile $x_1$, $x_2$, ..., $x_p$ within a certain class, \n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $P(x_1, x_2, ..., x_p | C_i)$,\n",
    "</p>\n",
    "\n",
    "is well approximated by the product of the individual conditional probabilities\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $P(x_1 | C_i ) × P(x_2 | C_i) × ... × P(x_p | C_i)$.\n",
    "</p>\n",
    "\n",
    "These two quantities are identical when the predictors are independent within each class.\n",
    "\n",
    "For example, suppose that \"lost money last year\" is an additional variable in the accounting fraud example. The simplifying assumption we make with naive Bayes is that, within a given class, we no longer need to look for the records characterized both by \"prior legal trouble\" and \"lost money last year\". Rather, assuming that the two are\n",
    "independent, we can simply multiply the probability of \"prior legal trouble\" by the probability of \"lost money last year\". Of course, complete independence is unlikely in practice, where some correlation between predictors is expected.\n",
    "\n",
    "In practice, despite the assumption violation, the procedure works quite well - primarily because what is usually needed is not a propensity for each record that is accurate in absolute terms but just a reasonably accurate *rank ordering* of propensities. Even when the assumption is violated, the rank ordering of the records' propensities is typically preserved.\n",
    "\n",
    "Note that if all we are interested in is a rank ordering, and the denominator remains the same for all classes, it is sufficient to concentrate only on the numerator. The disadvantage of this approach is that the probability values it yields (the propensities), while ordered correctly, are not on the same scale as the exact values that the user\n",
    "would anticipate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6398ecb6",
   "metadata": {},
   "source": [
    "### Using the Cutoff Probability Method\n",
    "\n",
    "The above procedure is for the basic case where we seek maximum classification accuracy for all classes. In the case of the *relatively rare class of special interest*, the procedure is:\n",
    "\n",
    "1. Establish a cutoff probability for the class of interest above which we consider that a record belongs to that class.\n",
    "2. For the class of interest, compute the probability that each individual predictor value in the record to be classified occurs in the training data.\n",
    "3. Multiply these probabilities times each other, then times the proportion of records belonging to the class of interest.\n",
    "4. Estimate the probability for the class of interest by taking the value calculated in Step 3 for the class of interest and dividing it by the sum of the similar values for all classes.\n",
    "5. If this value falls above the cutoff, assign the new record to the class of interest, otherwise not.\n",
    "6. Adjust the cutoff value as needed, as a parameter of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e92c1e",
   "metadata": {},
   "source": [
    "## Example 2: Predicting Fraudulent Financial Reports, Two Predictors\n",
    "\n",
    "Let us expand the financial reports example to two predictors, and, using a small subset of data, compare the complete (exact) Bayes calculations to the naive Bayes calculations.\n",
    "\n",
    "Consider the 10 customers of the accounting firm listed below. For each customer, we have information on whether it had prior legal trouble, whether it is a small or large company, and whether the financial report was found to be fraudulent or truthful. Using this information, we will calculate the conditional probability of fraud, given each of the four possible combinations $\\{\\text{y}, \\text{small}\\}$, $\\{\\text{y}, \\text{large}\\}$, $\\{\\text{n}, \\text{small}\\}$, $\\{\\text{n}, \\text{large}\\}$.\n",
    "\n",
    "| Company           | Prior legal trouble| Company Size     | Status     | \n",
    "|:------------------|:-------------------|:-----------------|:-----------|\n",
    "| 1                 | Yes                | Small            | Truthful   |\n",
    "| 2                 | No                 | Small            | Truthful   |\n",
    "| 3                 | No                 | Large            | Truthful   |\n",
    "| 4                 | No                 | Large            | Truthful   |\n",
    "| 5                 | No                 | Small            | Truthful   |\n",
    "| 6                 | No                 | Small            | Truthful   |\n",
    "| 7                 | Yes                | Small            | Fraudulent |\n",
    "| 8                 | Yes                | Large            | Fraudulent |\n",
    "| 9                 | No                 | Large            | Fraudulent |\n",
    "| 10                | Yes                | Large            | Fraudulent |\n",
    "\n",
    "**Complete (Exact) Bayes Calculations**\n",
    "\n",
    "The probabilities are computed as:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    <br>$P(\\text{fraudulent} ∣ \\text{Prior legal = y}, \\text{Size = small}) = \\frac{1}{2} = 0.5$\n",
    "    <br>$P(\\text{fraudulent} ∣ \\text{Prior legal = y}, \\text{Size = large}) = \\frac{2}{2} = 1$\n",
    "    <br>$P(\\text{fraudulent} ∣ \\text{Prior legal = n}, \\text{Size = small}) = \\frac{0}{3} = 0$\n",
    "    <br>$P(\\text{fraudulent} ∣ \\text{Prior legal = n}, \\text{Size = large}) = \\frac{1}{3} = 0.33$\n",
    "</p>\n",
    "\n",
    "**Naive Bayes Calculations**\n",
    "\n",
    "Now we compute the naive Bayes probabilities. For the conditional probability of fraudulent behaviors given $\\{\\text{prior legal = y}, \\text{Size = small}\\}$ the numerator is a multiplication of the proportion of $\\{\\text{Prior legal = y}\\}$ instances among the fraudulent companies, times the proportion of $\\{\\text{Size = small}\\}$ instances among the fraudulent companies, times the proportion of fraudulent companies:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $\\frac{3}{4} \\times \\frac{1}{4} \\times \\frac{4}{10} = 0.075$\n",
    "</p>\n",
    "\n",
    "To get the actual probabilities, we must also compute the numerator for the conditional probability of truthful behaviors given $\\{\\text{Prior Legal = y}, \\text{Size = small}\\}$:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $\\frac{1}{6} \\times \\frac{4}{6} \\times \\frac{6}{10} = 0.067$\n",
    "</p>\n",
    "\n",
    "The denominator is then the sum of these two conditional probabilities:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $ 0.075 + 0.067 = 0.14$\n",
    "</p>\n",
    "\n",
    "The conditional probability of fraudulent behaviors given $\\{\\text{Prior legal = y}, \\text{Size = small}\\}$ is therefore:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $\\frac{0.075}{0.14} = 0.53$\n",
    "</p>\n",
    "\n",
    "In a similar fashion, we compute all four conditional probabilities:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    <br>$P_{nb}(\\text{fraudulent} ∣ \\text{Prior legal = y}, \\text{Size = small}) = \\frac{(\\frac{3}{4})(\\frac{1}{4})(\\frac{6}{10})}{(\\frac{3}{4})(\\frac{1}{4})(\\frac{3}{4}) + (\\frac{1}{6})(\\frac{4}{6})(\\frac{6}{10})} = 0.53$\n",
    "    <br>$P_{nb}(\\text{fraudulent} ∣ \\text{Prior legal = y}, \\text{Size = large}) = 0.87$\n",
    "    <br>$P_{nb}(\\text{fraudulent} ∣ \\text{Prior legal = n}, \\text{Size = small}) = 0.07$\n",
    "    <br>$P_{nb}(\\text{fraudulent} ∣ \\text{Prior legal = n}, \\text{Size = large}) = 0.31$\n",
    "</p>\n",
    "\n",
    "Note how close these naive Bayes probabilities are to the exact Bayes probabilities. Although they are not equal, both would lead to exactly the same classification for a cutoff of 0.5 (and many other values). It is often the case that the rank ordering of probabilities is even closer to the exact Bayes method than the probabilities themselves, and for classification purposes it is the rank orderings that matter.\n",
    "\n",
    "We now consider a larger numerical example, where information on flights is used to predict flight delays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4766b636",
   "metadata": {},
   "source": [
    "## Example 3: Predicting Delayed Flights\n",
    "\n",
    "Predicting flight delays can be useful to a variety of organizations: airport authorities, airlines, and aviation authorities. At times, joint task forces have been formed to address the problem. If such an organization were to provide ongoing real-time assistance with flight delays, it would benefit from some advance notice about flights that are likely to be delayed.\n",
    "\n",
    "In this simplified illustration, we look at five predictors (see below). The outcome of interest is whether or not the flight is delayed (*delayed* here means arrived more than 15 minutes late). Our data consist of all flights from the Washington, DC area into the New York City area during January 2004. A record is a particular flight. The percentage of delayed flights among these 2201 flights is 19.5%. The data were obtained from the Bureau of Transportation Statistics (available at www.transtats.bts.gov). The goal is to accurately predict whether or not a new flight (not in this dataset), will be delayed. The outcome variable is whether the flight was delayed or not (1 = delayed and 0 = on time). In addition, information is collected on the predictors listed next:\n",
    "\n",
    "    Day of week: Coded as 1 = Monday, 2 = Tuesday, ..., 7 = Sunday\n",
    "    Sch. dep. time: Broken down into 18 intervals between 6:00 AM and 10:00 PM \n",
    "    Origin: Three airport codes: DCA (Reagan National), IAD (Dulles), BWI (Baltimore–Washington Int'l)\n",
    "    Destination: Three airport codes: JFK (Kennedy), LGA (LaGuardia), EWR (Newark)\n",
    "    Carrier: Eight airline codes: CO (Continental), DH (Atlantic Coast), DL (Delta),\n",
    "             MQ (American Eagle), OH (Comair), RU (Continental Express), UA (United), and US (USAirways)\n",
    "\n",
    "After converting all predictors to categorical and creating dummies, the data were partitioned into training (60%) and validation (40%) sets, and then a naive Bayes classifier was applied to the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6762a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_df = pd.read_csv(\"../datasets/FlightDelays.csv\")\n",
    "\n",
    "# convert to categorical\n",
    "delays_df.DAY_WEEK = delays_df.DAY_WEEK.astype(\"category\")\n",
    "delays_df[\"Flight Status\"] = delays_df[\"Flight Status\"].astype(\"category\")\n",
    "\n",
    "# create hourly bins departure\n",
    "delays_df.CRS_DEP_TIME = [round(t/100) for t in delays_df.CRS_DEP_TIME]\n",
    "delays_df.CRS_DEP_TIME = delays_df.CRS_DEP_TIME.astype(\"category\")\n",
    "\n",
    "predictors = [\"DAY_WEEK\", \"CRS_DEP_TIME\", \"ORIGIN\", \"DEST\", \"CARRIER\"]\n",
    "outcome = \"Flight Status\"\n",
    "\n",
    "X = pd.get_dummies(delays_df[predictors])\n",
    "y = delays_df[\"Flight Status\"].astype(\"category\")\n",
    "classes = list(y.cat.categories)\n",
    "\n",
    "# split into training and validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.40, random_state=1)\n",
    "\n",
    "# run naive Bayes\n",
    "delays_nb = MultinomialNB(alpha=0.01)\n",
    "delays_nb.fit(X_train, y_train)\n",
    "\n",
    "# predict probabilities\n",
    "pred_proba_train = delays_nb.predict_proba(X_train)\n",
    "pred_proba_valid = delays_nb.predict_proba(X_valid)\n",
    "\n",
    "# predict class membership\n",
    "y_train_pred = delays_nb.predict(X_train)\n",
    "y_valid_pred = delays_nb.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d285befd",
   "metadata": {},
   "source": [
    "Before using the output, let's see how the algorithm works. We start by generating pivot tables for the outcome vs. each of the five predictors using the training set, in order to obtain conditional probabilities (table below). Note that in this example, there are no  predictor values that were not represented in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ebcbb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ontime     0.8023\n",
      "delayed    0.1977\n",
      "Name: Flight Status, dtype: float64\n",
      "\n",
      "DAY_WEEK            1       2       3       4       5      6       7\n",
      "Flight Status                                                       \n",
      "delayed        0.1916  0.1494  0.1149  0.1264  0.1877  0.069  0.1609\n",
      "ontime         0.1246  0.1416  0.1445  0.1794  0.1690  0.136  0.1048\n",
      "\n",
      "CRS_DEP_TIME        6       7       8       9      10      11      12      13  \\\n",
      "Flight Status                                                                   \n",
      "delayed        0.0345  0.0536  0.0651  0.0192  0.0307  0.0115  0.0498  0.0460   \n",
      "ontime         0.0623  0.0633  0.0850  0.0567  0.0519  0.0340  0.0661  0.0746   \n",
      "\n",
      "CRS_DEP_TIME       14      15      16      17      18      19      20      21  \n",
      "Flight Status                                                                  \n",
      "delayed        0.0383  0.2031  0.0728  0.1533  0.0192  0.0996  0.0153  0.0881  \n",
      "ontime         0.0576  0.1171  0.0774  0.1001  0.0349  0.0397  0.0264  0.0529  \n",
      "\n",
      "ORIGIN            BWI     DCA     IAD\n",
      "Flight Status                        \n",
      "delayed        0.0805  0.5211  0.3985\n",
      "ontime         0.0604  0.6478  0.2918\n",
      "\n",
      "DEST              EWR     JFK     LGA\n",
      "Flight Status                        \n",
      "delayed        0.3793  0.1992  0.4215\n",
      "ontime         0.2663  0.1558  0.5779\n",
      "\n",
      "CARRIER            CO      DH      DL      MQ      OH      RU      UA      US\n",
      "Flight Status                                                                \n",
      "delayed        0.0575  0.3142  0.0958  0.2222  0.0077  0.2184  0.0153  0.0690\n",
      "ontime         0.0349  0.2295  0.2040  0.1171  0.0104  0.1690  0.0170  0.2181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split the original data frame into a train and test using the same\n",
    "# random_state\n",
    "train_df, valid_df = train_test_split(delays_df, test_size=0.40, random_state=1)\n",
    "\n",
    "pd.set_option(\"precision\", 4)\n",
    "# probability of flight status\n",
    "print(train_df[\"Flight Status\"].value_counts() / len(train_df))\n",
    "print()\n",
    "\n",
    "for predictor in predictors:\n",
    "    # construct the frequency table\n",
    "    df = train_df[[\"Flight Status\", predictor]]\n",
    "    freq_table = df.pivot_table(index=\"Flight Status\", columns=predictor, aggfunc=len)\n",
    "\n",
    "    # divide each value by the sum of the row to get conditional probabilities\n",
    "    prop_table = freq_table.apply(lambda x: x/sum(x), axis=1)\n",
    "    print(prop_table)\n",
    "    print()\n",
    "\n",
    "pd.reset_option(\"precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a6543c",
   "metadata": {},
   "source": [
    "To classify a new flight, we compute the probability that it will be delayed and the probability that it will be on time. Recall that since both probabilities will have the same denominator, we can just compare the numerators. Each numerator is computed by multiplying all the conditional probabilities of the relevant predictor values and, finally, multiplying by the proportion of that class (in this case $\\hat{P}$(delayed) = 0.2). Let us use an example: to classify a Delta flight from DCA to LGA departing between 10:00 AM and 11:00 AM on a Sunday, we first compute the numerators using the values from the pivot tables:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $\\hat{P}(\\text{delayed} ∣ \\text{Carrier=DL}, \\text{Day_Week}=7, \\text{Dep_Time}=10, \\text{Dest=LGA}, \\text{Origin=DCA}) = \\propto (0.2)(0.0958)(0.1609)(0.4215)(0.0307)(0.5211) = 0.000021$\n",
    "    <br>$\\hat{P}(\\text{ontime} ∣ \\text{Carrier=DL}, \\text{Day_Week}=7, \\text{Dep_Time}=10, \\text{Dest=LGA}, \\text{Origin=DCA}) = \\propto (0.8)(0.2040)(0.1048)(0.05779)(0.0519)(0.6478) = 0.000033$\n",
    "</p>\n",
    "\n",
    "The symbol $\\propto$ means \"is proportional to\", reflecting the fact that this calculation deals only with the numerator in the naive Bayes formula. Comparing the numerators, it is therefore, more likely that the flight will be on time. Note that a record with such a combination of predictor values does not exist in the training set, and therefore we use the naive Bayes rather than the exact Bayes. To compute the actual probability, we divide each of the numerators by their sum:\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    $\\hat{P}(\\text{delayed} ∣ \\text{Carrier=DL}, \\text{Day_Week}=7, \\text{Dep_Time}=10, \\text{Dest=LGA}, \\text{Origin=DCA}) = \\frac{0.000021}{0.000021+0.000033} = 0.058$,\n",
    "    <br>$\\hat{P}(\\text{ontime} ∣ \\text{Carrier=DL}, \\text{Day_Week}=7, \\text{Dep_Time}=10, \\text{Dest=LGA}, \\text{Origin=DCA}) = \\frac{0.000033}{0.000021+0.000033} = 0.942$\n",
    "</p>\n",
    "\n",
    "Of course, we rely on software to compute these probabilities for any records of interest (in the training set, the validation set, or for scoring new data). The following table shows the predicted probability and class for the example flight, which coincide with our manual calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5babd1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>ontime</td>\n",
       "      <td>ontime</td>\n",
       "      <td>0.057989</td>\n",
       "      <td>0.942011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual predicted         0         1\n",
       "1225  ontime    ontime  0.057989  0.942011"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classify a specifi flight by searching in the dataset\n",
    "# for a flight with the same predictor values\n",
    "df = pd.concat([pd.DataFrame({\"actual\": y_valid, \"predicted\": y_valid_pred}),\n",
    "                pd.DataFrame(pred_proba_valid, index=y_valid.index)], axis=1)\n",
    "\n",
    "mask = ((X_valid.CARRIER_DL == 1) & (X_valid.DAY_WEEK_7 == 1) &\n",
    "        (X_valid.CRS_DEP_TIME_10 == 1) & (X_valid.DEST_LGA == 1) &\n",
    "        (X_valid.ORIGIN_DCA == 1))\n",
    "df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aea5cc0",
   "metadata": {},
   "source": [
    "Finally, to evaluate the performance of the naive Bayes classifier for our data, we can use the confusion matrix, gains and lift charts, and all the measures that were described in [Evaluating Predictive Performance](evaluating-predictive-performance.ipynb). For our example, the confusion matrices for the training and validation sets are shown below. We see that the overall accuracy level is around 80% for both the training and validation data. In comparison, a naive rule that would classify all 880 flights in the validation set as \"on time\" would have missed the 172 delayed flights, also resulting in a 80% accuracy. Thus, by a simple accuracy measure, the naive Bayes model does no better than the naive rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d7e16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Accuracy 0.7955)\n",
      "\n",
      "        Prediction\n",
      " Actual delayed  ontime\n",
      "delayed      52     209\n",
      " ontime      61     998\n",
      "\n",
      "Confusion Matrix (Accuracy 0.7821)\n",
      "\n",
      "        Prediction\n",
      " Actual delayed  ontime\n",
      "delayed      26     141\n",
      " ontime      51     663\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "classification_summary(y_train, y_train_pred, class_names=classes)\n",
    "print()\n",
    "\n",
    "# validation\n",
    "classification_summary(y_valid, y_valid_pred, class_names=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec1a7dd",
   "metadata": {},
   "source": [
    "However, examining the gains and lift charts shows the strength of the naive Bayes in capturing the delayed flights effectively, when the goal is ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a488cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"actual\": 1 - y_valid.cat.codes, \"prob\": pred_proba_valid[:, 0]})\n",
    "df = df.sort_values(by=[\"prob\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "gains_chart(df.actual, ax=ax)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
