{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "696e11e7",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "This notebook explains linear regression models for the purpose of prediction. It discusses the differences between fitting and using regression models for the purpose of inference (as in classical statistics) and for prediction. A predictive goal calls for evaluating model performance on a validation set, and for using predictive metrics. It then raises the challenges of using many predictors and describes variable selection algorithms that are often implemented in linear regression procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e89764",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Imports required for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f78b5627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, LassoCV, BayesianRidge\n",
    "from dmutils import regression_summary\n",
    "from dmutils import adjusted_r2_score, AIC_score\n",
    "from dmutils import exhaustive_search, backward_elimination\n",
    "from dmutils import forward_selection, stepwise_selection\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf194e7e",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The most popular model for making predictions is the *multiple linear regression model* encountered in most introductory statistics courses and textbooks. This model is used to fit a relationship between a numerical *outcome variable Y* (also called the *response*, *target*, or *dependent variable*) and a set of *predictors* $X_1$, $X_2$, ..., $X_p$ (also referred to as *independent variables*, *input variables*, *regressors*, or *covariates*). The assumption is that the following function approximates the relationship between the predictors and outcome variable:\n",
    "\n",
    "\n",
    "<p>\n",
    "    <center>\n",
    "        $Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\epsilon$\n",
    "    </center>\n",
    "</p>\n",
    "\n",
    "where $\\beta_0$, ..., $\\beta_p$ are *coefficients* and $\\epsilon$ is the *noise* or *unexplained* part. Data are then used to estimate the coefficients and to quantify the noise. In predictive modeling, the data are also used to evaluate model performance.\n",
    "\n",
    "Regression modeling means not only estimating the coefficients but also choosing which predictors to include and in what form. For example, a numerical predictor can be included as is, or in logarithmic form $[\\log (X)]$, or in a binned form (e.g., age group). Choosing the right form depends on domain knowledge, data availability, and needed predictive power.\n",
    "\n",
    "Multiple linear regression is applicable to numerous predictive modeling situations. Examples are predicting customer activity on credit cards from their demographics and historical activity patterns, predicting expenditures on vacation travel based on historical frequent flyer data, predicting staffing requirements at help desks based on historical data and product and sales information, predicting sales from cross-selling of products from historical information, and predicting the impact of discounts on sales in retail outlets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c92483",
   "metadata": {},
   "source": [
    "## Exploratory vs. Predictive modeling\n",
    "\n",
    "Before introducing the use of linear regression for prediction, one must clarify an important distinction that often escapes those with earlier familiarity with linear regression from courses in statistics. In particular, the two popular but different objectives behind fitting a regression model are:\n",
    "\n",
    "1. Explaining or quantifying the average effect of inputs on an outcome (explanatory or descriptive task, respectively)\n",
    "\n",
    "2. Predicting the outcome value for new records, given their input values (predictive task)\n",
    "\n",
    "The classical statistical approach is focused on the first objective. In that scenario, the data are treated as a random sample from a larger population of interest. The regression model estimated from this sample is an attempt to capture the *average* relationship in the larger population. This model is then used in decision-making to generate statements such as \"a unit increase in service speed ($X_1$) is associated with an average increase of 5 points in customer satisfaction ($Y$), all other factors ($X_2$, $X_3$, ..., $X_p$) being equal.\". If $X_1$ is known to cause $Y$, then such a statement indicates actionable policy changes - this is called explanatory modeling. When the causal structure is unknown, then this model quantifies the degree of* association* between the inputs and outcome variable, and the approach is called descriptive modeling.\n",
    "\n",
    "In predictive analytics, however, the focus is typically on the second goal: predicting new individual records. Here we are not interested in the coefficients themselves, nor in the \"average record\", but rather in the predictions that this model can generate for new records. In this scenario, the model is used for micro-decision-making at the record level. As a practical example, it could be used to predict customer satisfaction for each new customer of interest.\n",
    "\n",
    "Both explanatory and predictive modeling involve using a dataset to fit a model (i.e., to estimate coefficients), checking model validity, assessing its performance, and comparing to other models. However, the modeling steps and performance assessment differ in the two cases, usually leading to different final models. Therefore, the choice of model is closely tied to whether the goal is explanatory or predictive.\n",
    "\n",
    "In explanatory and descriptive modeling, where the focus is on modeling the average record, the attempt is to fit the best model to the data to learn about the underlying relationship in the population. In contrast, in predictive modeling (data mining), the goal is to find a regression model that best predicts new individual records. A regression model that fits the existing data too well is not likely to perform well with new data. Hence, we look for a model that has the highest predictive power by evaluating it on a holdout set and using predictive metrics.\n",
    "\n",
    "Let us summarize the main differences in using a linear regression in the two scenarios:\n",
    "\n",
    "1. A good explanatory model is one that fits the data closely, whereas a good predictive model is one that predicts new records accurately. Choices of input variables and their form can therefore differ.\n",
    "\n",
    "2. In explanatory models, the entire dataset is used for estimating the best-fit model, to maximize the amount of information that we have about the hypothesized relationship in the population. When the goal is to predict outcomes of new individual records, the data are typically split into a training set and a validation set. The training set is used to estimate the model, and the validation or *holdout set* is used to assess this model's predictive performance on new, unobserved data.\n",
    "\n",
    "3. Performance measures for explanatory models measure how close the data fit the model (how well the model approximates the data) and how strong the average relationship is, whereas in predictive models performance is measured by predictive accuracy (how well the model predicts new individual records).\n",
    "\n",
    "4. In explanatory models the focus is on the coefficients ($\\beta$), whereas in predictive models the focus is on the predictions ($\\hat{y}$).\n",
    "\n",
    "For these reasons, it is extremely important to know the goal of the analysis before beginning the modeling process. A good predictive model can have a looser fit to the data on which it is based, and a good explanatory model can have low prediction accuracy. Therefore, the remainder of this notebook focuses on predictive models because these are more popular in data mining and because most statistics textbooks focus on explanatory modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82937148",
   "metadata": {},
   "source": [
    "## Estimating the Regression Equation and Prediction\n",
    "\n",
    "Once determined the predictors to include and their form, we estimate the coefficients of the regression formula from the data using a method called *ordinary least squares* (OLS). This method finds values $\\hat{\\beta}_0$, $\\hat{\\beta}_1$, $\\hat{\\beta}_2$, ..., $\\hat{\\beta}_p$ that minimize the sum of squared deviations between the actual outcome values ($Y$) and their predicted values based on that model ($\\hat{Y}$).\n",
    "\n",
    "To predict the value of the outcome variable for a record with predictor values $x_1$, $x_2$, ..., $x_p$, we use the equation:\n",
    "\n",
    "<p>\n",
    "    <center>\n",
    "        $\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + ... + \\hat{\\beta}_p x_p$\n",
    "    </center>\n",
    "</p>\n",
    "\n",
    "Predictions based on this equation are the best predictions possible in the sense that they will be unbiased (equal to the true values on average) and will have the smallest mean squared error compared to any unbiased estimates *if* we make the following assumptions:\n",
    "\n",
    "1. The noise $\\epsilon$ (or equivalently, $Y$) follows a normal distribution.\n",
    "\n",
    "2. The choice of predictors and their form is correct (*linearity*).\n",
    "\n",
    "3. The records are independent of each other.\n",
    "\n",
    "4. The variability in the outcome values for a given set of predictors is the same regardless of the values of the predictors (*homoskedasticity*).\n",
    "\n",
    "An important and interesting fact for the predictive goal is that *even if we drop the first assumption and allow the noise to follow an arbitrary distribution, these estimates are very good for prediction*, in the sense that among all linear models, as defined by equation above, the model using the least squares estimates, $\\hat{\\beta}_0$, $\\hat{\\beta}_1$, $\\hat{\\beta}_2$, ..., $\\hat{\\beta}_p$, will have the smallest mean squared errors. The assumption of a normal distribution is required in explanatory modeling, where it is used for constructing confidence intervals and statistical tests for the model parameters.\n",
    "\n",
    "Even if the other assumptions are violated, it is still possible that the resulting predictions are sufficiently accurate and precise for the purpose they are intended for. The key is to evaluate predictive performance of the model, which is the main priority. Satisfying assumptions is of secondary interest and residual analysis can give clues to potential improved models to examine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fbb641",
   "metadata": {},
   "source": [
    "### Example: Predicting the Price of Used Toyota Corolla Cars\n",
    "\n",
    "A large Toyota car dealership offers purchasers of new Toyota cars the option to buy their used car as part of a trade-in. In particular, a new promotion promises to pay high prices for used Toyota Corolla cars for purchasers of a new car. The dealer then sells the used cars for a small profit. To ensure a reasonable profit, the dealer needs to be able to predict the price that the dealership will get for the used cars. For that reason, data were collected on all previous sales of used Toyota Corollas at the dealership. The data include the sales price and other information on the car, such as its age, mileage, fuel type, and engine size. A description of each of these variables is given below:\n",
    "\n",
    "\n",
    "    Price: Offer price in Euros\n",
    "    Age: Age in months as of August 2004\n",
    "    Kilometers: Accumulated kilometers on odometer\n",
    "    Fuel type: Fuel type (Petrol, Diesel, CNG)\n",
    "    HP: Horsepower\n",
    "    Metallic: Metallic color? (Yes = 1, No = 0)\n",
    "    Automatic: Automatic (Yes = 1, No = 0)\n",
    "    CC: Cylinder volume in cubic centimeters\n",
    "    Doors: Number of doors\n",
    "    QuartTax: Quarterly road tax in Euros\n",
    "    Weight: Weight in kilograms\n",
    "\n",
    "A sample of this dataset is shown below. The total number of records in the dataset is 1000 cars (we use the first 1000 cars from the dataset `ToyotoCorolla.csv`).After partitioning the data into training (60%) and validation\n",
    "(40%) sets, we fit a multiple linear regression model between price (the outcome variable) and the other variables (as predictors) using only the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96259917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Model</th>\n",
       "      <th>Price</th>\n",
       "      <th>Age_08_04</th>\n",
       "      <th>Mfg_Month</th>\n",
       "      <th>Mfg_Year</th>\n",
       "      <th>KM</th>\n",
       "      <th>Fuel_Type</th>\n",
       "      <th>HP</th>\n",
       "      <th>Met_Color</th>\n",
       "      <th>...</th>\n",
       "      <th>Powered_Windows</th>\n",
       "      <th>Power_Steering</th>\n",
       "      <th>Radio</th>\n",
       "      <th>Mistlamps</th>\n",
       "      <th>Sport_Model</th>\n",
       "      <th>Backseat_Divider</th>\n",
       "      <th>Metallic_Rim</th>\n",
       "      <th>Radio_cassette</th>\n",
       "      <th>Parking_Assistant</th>\n",
       "      <th>Tow_Bar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors</td>\n",
       "      <td>13500</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>2002</td>\n",
       "      <td>46986</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors</td>\n",
       "      <td>13750</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>2002</td>\n",
       "      <td>72937</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors</td>\n",
       "      <td>13950</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>2002</td>\n",
       "      <td>41711</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors</td>\n",
       "      <td>14950</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>2002</td>\n",
       "      <td>48000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>TOYOTA Corolla 2.0 D4D HATCHB SOL 2/3-Doors</td>\n",
       "      <td>13750</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>2002</td>\n",
       "      <td>38500</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                          Model  Price  Age_08_04  \\\n",
       "0   1  TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors  13500         23   \n",
       "1   2  TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors  13750         23   \n",
       "2   3  TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors  13950         24   \n",
       "3   4  TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors  14950         26   \n",
       "4   5    TOYOTA Corolla 2.0 D4D HATCHB SOL 2/3-Doors  13750         30   \n",
       "\n",
       "   Mfg_Month  Mfg_Year     KM Fuel_Type  HP  Met_Color  ... Powered_Windows  \\\n",
       "0         10      2002  46986    Diesel  90          1  ...               1   \n",
       "1         10      2002  72937    Diesel  90          1  ...               0   \n",
       "2          9      2002  41711    Diesel  90          1  ...               0   \n",
       "3          7      2002  48000    Diesel  90          0  ...               0   \n",
       "4          3      2002  38500    Diesel  90          0  ...               1   \n",
       "\n",
       "   Power_Steering  Radio  Mistlamps  Sport_Model  Backseat_Divider  \\\n",
       "0               1      0          0            0                 1   \n",
       "1               1      0          0            0                 1   \n",
       "2               1      0          0            0                 1   \n",
       "3               1      0          0            0                 1   \n",
       "4               1      0          1            0                 1   \n",
       "\n",
       "   Metallic_Rim  Radio_cassette  Parking_Assistant  Tow_Bar  \n",
       "0             0               0                  0        0  \n",
       "1             0               0                  0        0  \n",
       "2             0               0                  0        0  \n",
       "3             0               0                  0        0  \n",
       "4             0               0                  0        0  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce data frame to the top 1000 rows and select columns for regression analysis\n",
    "car_df = pd.read_csv(\"../datasets/ToyotaCorolla.csv\")\n",
    "car_df = car_df.iloc[0:1000]\n",
    "\n",
    "car_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35604dc",
   "metadata": {},
   "source": [
    "Below are the coefficients of the Multiple Linear Regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbb8ce6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Predictor  coefficient\n",
      "0          Age_08_04  -140.748761\n",
      "1                 KM    -0.017840\n",
      "2                 HP    36.103419\n",
      "3          Met_Color    84.281830\n",
      "4          Automatic   416.781954\n",
      "5                 CC     0.017737\n",
      "6              Doors   -50.657863\n",
      "7      Quarterly_Tax    13.625325\n",
      "8             Weight    13.038711\n",
      "9   Fuel_Type_Diesel  1066.464681\n",
      "10  Fuel_Type_Petrol  2310.249543\n",
      "\n",
      "Regression statistics\n",
      "\n",
      "                      Mean Error (ME) : 0.0000\n",
      "       Root Mean Squared Error (RMSE) : 1400.5823\n",
      "            Mean Absolute Error (MAE) : 1046.9072\n",
      "          Mean Percentage Error (MPE) : -1.0223\n",
      "Mean Absolute Percentage Error (MAPE) : 9.2994\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Age_08_04\", \"KM\", \"Fuel_Type\", \"HP\", \"Met_Color\", \"Automatic\", \"CC\",\n",
    "              \"Doors\", \"Quarterly_Tax\", \"Weight\"]\n",
    "outcome = \"Price\"\n",
    "\n",
    "# partition data\n",
    "X = pd.get_dummies(car_df[predictors], drop_first=True)\n",
    "y = car_df[outcome]\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X, y, \n",
    "                                                      test_size=0.4,\n",
    "                                                      random_state=1)\n",
    "car_lmr = LinearRegression()\n",
    "car_lmr.fit(train_X, train_y)\n",
    "\n",
    "# print coefficients\n",
    "print(pd.DataFrame({'Predictor': X.columns, 'coefficient': car_lmr.coef_}))\n",
    "# print performance measures (training data)\n",
    "regression_summary(train_y, car_lmr.predict(train_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa50c415",
   "metadata": {},
   "source": [
    "Notice that the Fuel Type predictor has three categories (Petrol, Diesel, and CNG). We therefore have two dummy variables in the model: Fuel_Type_Petrol (0/1) and Fuel_Type_Diesel (0/1); the third, for CNG (0/1), is redundant given the information on the first two dummies. Including the redundant dummy would cause the regression to fail, since the redundant dummy will be a perfect linear combination of the other two.\n",
    "\n",
    "The regression coefficients are then used to predict prices of individual used Toyota Corolla cars based on their age, mileage, and so on. Below is a sample of predicted prices for 20 cars in the validation set, using the estimated model. It gives the predictions and their errors (relative to the actual prices) for these 20 cars. Below the predictions, we have overall measures of predictive accuracy. Note that the mean\n",
    "error (ME) is \\\\$104 and RMSE = \\\\$1,313."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b93667c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Predicted  Actual     Residual\n",
      "507  10607.333940   11500   892.666060\n",
      "818   9272.705792    8950  -322.705792\n",
      "452  10617.947808   11450   832.052192\n",
      "368  13600.396275   11450 -2150.396275\n",
      "242  12396.694660   11950  -446.694660\n",
      "929   9496.498212    9995   498.501788\n",
      "262  12480.063217   13500  1019.936783\n",
      "810   8834.146068    7950  -884.146068\n",
      "318  12183.361282    9900 -2283.361282\n",
      "49   19206.965683   21950  2743.034317\n",
      "446  10987.498309   11950   962.501691\n",
      "142  18501.527375   19950  1448.472625\n",
      "968   9914.690947    9950    35.309053\n",
      "345  13827.299932   14950  1122.700068\n",
      "971   7966.732543   10495  2528.267457\n",
      "133  17185.242041   15950 -1235.242041\n",
      "104  19952.658062   19450  -502.658062\n",
      "6    16570.609280   16900   329.390720\n",
      "600  13739.409113   11250 -2489.409113\n",
      "496  11267.513740   11750   482.486260\n",
      "\n",
      "Regression statistics\n",
      "\n",
      "                      Mean Error (ME) : 103.6803\n",
      "       Root Mean Squared Error (RMSE) : 1312.8523\n",
      "            Mean Absolute Error (MAE) : 1017.5972\n",
      "          Mean Percentage Error (MPE) : -0.2633\n",
      "Mean Absolute Percentage Error (MAPE) : 9.0111\n"
     ]
    }
   ],
   "source": [
    "# Use predict() to make predictions on a new set\n",
    "car_lmr_pred = car_lmr.predict(valid_X)\n",
    "result = pd.DataFrame({\"Predicted\": car_lmr_pred,\n",
    "                       \"Actual\": valid_y,\n",
    "                       \"Residual\": valid_y - car_lmr_pred})\n",
    "print(result.head(20))\n",
    "# print performance measures (validation data)\n",
    "regression_summary(valid_y, car_lmr_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc9db98",
   "metadata": {},
   "source": [
    "A histogram of the residuals shows that most of the errors are between ± \\\\$2000. This error magnitude might be small relative to the car price, but should be taken into account when considering the profit. Another observation of interest is the large positive residuals (under-predictions), which may or may not be a concern, depending on the application. Measures such as the mean error, and error percentiles are used to assess the predictive performance of a model and to compare models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c8c33c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of datapoints with a residual in [-1406, 1406]:  0.7425\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_residuals = valid_y - car_lmr_pred\n",
    "\n",
    "# Determine the percentage of datapoints with a residual in [-1406, 1406] = approx. 75%\n",
    "print(\"Percentage of datapoints with a residual in [-1406, 1406]: \",\n",
    "      len(all_residuals[(all_residuals > -1406) & (all_residuals < 1406)]) / len(all_residuals))\n",
    "\n",
    "pd.DataFrame({\"Residuals\": all_residuals}).hist(bins=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a467c041",
   "metadata": {},
   "source": [
    "## Variable Selection in Linear Regression\n",
    "\n",
    "### Reducing the Number of Predictors\n",
    "\n",
    "A frequent problem in data mining is that of using a regression equation to predict the value of a dependent variable when we have many variables available to choose as predictors in our model. Given the high speed of modern algorithms for multiple linear regression calculations, it is tempting in such a situation to take a kitchen-sink approach: Why bother to select a subset? Just use all the variables in the model.\n",
    "\n",
    "Another consideration favoring the inclusions of numerous variables is the hope that a previously hidden relationship will emerge. For example, a company found that customers who had purchased anti-scuff protectors for chair and table legs had lower credit risks. However, there are several reasons for exercising caution before throwing all possible variables into a model.\n",
    "\n",
    "   - It may be expensive or not feasible to collect a full complement of predictors for future predictions.\n",
    "   - We may be able to measure fewer predictors more accurately (e.g., in surveys).\n",
    "   - The more predictors, the higher the chance of missing values in the data. If we delete or impute records with missing values, multiple predictors will lead to a higher rate of record deletion or imputation.\n",
    "   - *Parsimony* is an important property of good models. We obtain more insight into the influence of predictors in models with few parameters.\n",
    "   - Estimates of regression coefficients are likely to be unstable, due to *multicollinearity* in models with many variables. (Multicollinearity is the presence of two or more predictors sharing the same linear relationship with the outcome variable.) Regression coefficients are more stable for parsimonious models. One very rough rule of thumb is to have a number of records *n* larger than 5(*p* + 2), where *p* is the number of predictors.\n",
    "   - It can be shown that using predictors that are uncorrelated with the outcome variable increases the variance of predictions.\n",
    "   - It can be shown that dropping predictors that are actually correlated with the outcome variable can increase the average error (bias) of predictions.\n",
    "\n",
    "The last two points mean that there is a trade-off between too few and too many predictors. In general, accepting some bias can reduce the variance in predictions. This *bias-variance* trade-off is particularly important for large numbers of predictors, because in that case, it is very likely that there are variables in the model that have small coefficients relative to the standard deviation of the noise and also exhibit at least moderate correlation with other variables. Dropping such variables will improve the predictions, as it reduces the prediction variance. This type of bias-variance trade-off is a basic aspect of most data mining procedures for prediction and classification. In light of this, methods for reducing the number of predictors p to a smaller set are often used.\n",
    "\n",
    "### How to Reduce the Number of Predictors\n",
    "\n",
    "The first step in trying to reduce the number of predictors should always be to use domain knowledge. It is important to understand what the various predictors are measuring and why they are relevant for predicting the outcome variable. With this knowledge, the set of predictors should be reduced to a sensible set that reflects the\n",
    "problem at hand. Some practical reasons for predictor elimination are the expense of collecting this information in the future; inaccuracy; high correlation with another predictor; many missing values; or simply irrelevance. Also helpful in examining potential predictors are summary statistics and graphs, such as frequency and correlation tables, predictor-specific summary statistics and plots, and missing value counts.\n",
    "\n",
    "The next step makes use of computational power and statistical performance metrics. In general, there are two types of methods for reducing the number of predictors in a model. The first is an *exhaustive search* for the \"best\" subset of predictors by fitting regression models with all the possible combinations of predictors. The exhaustive search approach is not practical in many applications due to the large number of possible models. The second approach is to search through a partial set of models. We describe these two approaches next.\n",
    "\n",
    "#### Exhaustive Search\n",
    "\n",
    "The idea here is to evaluate all subsets of predictors. Since the number of subsets for even moderate values of *p* is very large, after the algorithm creates the subsets and runs all the models, we need some way to examine the most promising subsets and to select from them. The challenge is to select a model that is not too simplistic in terms of excluding important parameters (the model is *under-fit*), nor overly complex thereby modeling random noise (the model is *over-fit*). Several criteria for evaluating and comparing models are based on metrics computed from the training data:\n",
    "\n",
    "<p>\n",
    "    <center>\n",
    "        $R^{2}_{adj} = 1 -\\frac{n - 1}{n - p - 1} (1 - R^2)$,\n",
    "    </center>\n",
    "</p>\n",
    "\n",
    "where $R^2$ is the proportion of explained variability in the model (in a model with a single predictor, this is the squared correlation). Like $R^2$, higher values of $R^{2}_{adj}$ indicate better fit. Unlike $R^2$, which does not account for the number of predictors used, $R^{2}_{adj}$ uses a penalty on the number of predictors. This avoids the artificial increase in $R^2$ that can result from simply increasing the number of predictors but not the amount of information. It can be shown that using $R^{2}_{adj}$ to choose a subset is equivalent to choosing the subset that minimizes the training RMSE.\n",
    "\n",
    "A second popular set of criteria for balancing under-fitting and over-fitting are the *Akaike Information Criterion (AIC)* and *Schwartz’s Bayesian Information Criterion (BIC)*. AIC and BIC measure the goodness of fit of a model, but also include a penalty that is a function of the number of parameters in the model. As such, they can be used to compare various models for the same data set. *AIC* and *BIC* are estimates of prediction error based in information theory. For linear regression, AIC and BIC can be computed from the formulas:\n",
    "\n",
    "<p>\n",
    "    <center>\n",
    "        $$AIC = n \\ln(\\frac{SSE}{n}) + n (1 + \\ln(2\\pi)) + 2 (p + 1)$$,\n",
    "    </center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    <center>\n",
    "        $BIC = n \\ln(\\frac{SSE}{n}) + n (1 + \\ln(2\\pi)) + \\ln(n)(p + 1)$,\n",
    "    </center>\n",
    "</p>\n",
    "\n",
    "where SSE is the model's sum of squared errors. In general, models with smaller AIC and BIC values are considered better.\n",
    "\n",
    "Finally, a useful point to note is that for a fixed size of subset $R^{2}$, $R^{2}_{adj}$, AIC, and BIC\n",
    "all select the same subset. In fact, there is no difference between them in the order of merit they ascribe to subsets of a fixed size. This is good to know if comparing models with the same number of predictors, but often we want to compare models with different numbers of predictors.\n",
    "\n",
    "Following are the results of of applying an exhaustive search on the Toyota Corolla price data (with the 11 predictors). Because Python does not have an exhaustive search routine, we created a loop that iterates through all predictor combinations and within models with the same number of predictors, selects the models with the highest $R^{2}_{adj}$ (which is equivalent to choosing one of the other measures mentioned above). The code reports the best model with a single predictor, two predictors, and so on. It can be seen that the $R^{2}_{adj}$ increases until eight predictors are used and then slowly decreases. The AIC also indicates that a model with  eight predictors is good. The dominant predictor in all models is the age of the car, with horsepower, weight and mileage playing important roles as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e136301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>r2adj</th>\n",
       "      <th>AIC</th>\n",
       "      <th>Age_08_04</th>\n",
       "      <th>Automatic</th>\n",
       "      <th>CC</th>\n",
       "      <th>Doors</th>\n",
       "      <th>Fuel_Type_Diesel</th>\n",
       "      <th>Fuel_Type_Petrol</th>\n",
       "      <th>HP</th>\n",
       "      <th>KM</th>\n",
       "      <th>Met_Color</th>\n",
       "      <th>Quarterly_Tax</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.767901</td>\n",
       "      <td>10689.712094</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.801160</td>\n",
       "      <td>10597.910645</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.829659</td>\n",
       "      <td>10506.084235</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.846357</td>\n",
       "      <td>10445.174820</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.849044</td>\n",
       "      <td>10435.578836</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.853172</td>\n",
       "      <td>10419.932278</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.853860</td>\n",
       "      <td>10418.104025</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.854297</td>\n",
       "      <td>10417.290103</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.854172</td>\n",
       "      <td>10418.789079</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.854036</td>\n",
       "      <td>10420.330800</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.853796</td>\n",
       "      <td>10422.298278</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     n     r2adj           AIC  Age_08_04  Automatic     CC  Doors  \\\n",
       "0    1  0.767901  10689.712094       True      False  False  False   \n",
       "1    2  0.801160  10597.910645       True      False  False  False   \n",
       "2    3  0.829659  10506.084235       True      False  False  False   \n",
       "3    4  0.846357  10445.174820       True      False  False  False   \n",
       "4    5  0.849044  10435.578836       True      False  False  False   \n",
       "5    6  0.853172  10419.932278       True      False  False  False   \n",
       "6    7  0.853860  10418.104025       True      False  False  False   \n",
       "7    8  0.854297  10417.290103       True       True  False  False   \n",
       "8    9  0.854172  10418.789079       True       True  False   True   \n",
       "9   10  0.854036  10420.330800       True       True  False   True   \n",
       "10  11  0.853796  10422.298278       True       True   True   True   \n",
       "\n",
       "    Fuel_Type_Diesel  Fuel_Type_Petrol     HP     KM  Met_Color  \\\n",
       "0              False             False  False  False      False   \n",
       "1              False             False   True  False      False   \n",
       "2              False             False   True  False      False   \n",
       "3              False             False   True   True      False   \n",
       "4              False             False   True   True      False   \n",
       "5              False              True   True   True      False   \n",
       "6               True              True   True   True      False   \n",
       "7               True              True   True   True      False   \n",
       "8               True              True   True   True      False   \n",
       "9               True              True   True   True       True   \n",
       "10              True              True   True   True       True   \n",
       "\n",
       "    Quarterly_Tax  Weight  \n",
       "0           False   False  \n",
       "1           False   False  \n",
       "2           False    True  \n",
       "3           False    True  \n",
       "4            True    True  \n",
       "5            True    True  \n",
       "6            True    True  \n",
       "7            True    True  \n",
       "8            True    True  \n",
       "9            True    True  \n",
       "10           True    True  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_model(variables):\n",
    "    model = LinearRegression()\n",
    "    model.fit(train_X[list(variables)], train_y)\n",
    "    return model\n",
    "\n",
    "def score_model(model, variables):\n",
    "    pred_y = model.predict(train_X[list(variables)])\n",
    "    # we negate as score is optimized to be as low as possible\n",
    "    return -adjusted_r2_score(train_y, pred_y, model)\n",
    "\n",
    "all_variables = train_X.columns\n",
    "results = exhaustive_search(all_variables, train_model, score_model)\n",
    "\n",
    "data = []\n",
    "for result in results:\n",
    "    model = result[\"model\"]\n",
    "    variables = list(result[\"variables\"])\n",
    "    AIC = AIC_score(train_y, model.predict(train_X[variables]), model)\n",
    "\n",
    "    d = {\"n\": result[\"n\"], \"r2adj\": -result[\"score\"], \"AIC\": AIC}\n",
    "    d.update({var: var in result[\"variables\"] for var in sorted(all_variables)})\n",
    "    data.append(d)\n",
    "\n",
    "pd.DataFrame(data, columns=(\"n\", \"r2adj\", \"AIC\") + tuple(sorted(all_variables)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe41d13",
   "metadata": {},
   "source": [
    "#### Popular Subset Selection Algorithms\n",
    "\n",
    "The second method of finding the best subset of predictors relies on a partial, iterative search through the space of all possible regression models. The end product is one best subset of predictors (although there do exist variations of these methods that identify several close-to-best choices for different sizes of predictor subsets). This approach is computationally cheaper, but it has the potential of missing \"good\" combinations of predictors. None of the methods guarantee that they yield the best subset for any criterion, such as $R^{2}_{adj}$. They are reasonable methods for situations with a large number of predictors, but for a moderate number of predictors, the exhaustive search is preferable.\n",
    "\n",
    "Three popular iterative search algorithms are *forward selection*, *backward elimination*, and *stepwise regression*. In *forward selection*, we start with no predictors and then add predictors one by one. Each predictor added is the one (among all predictors) that has the largest contribution to $R^{2}$ on top of the predictors that are already in it. The algorithm stops when the contribution of additional predictors is not\n",
    "statistically significant. The main disadvantage of this method is that the algorithm will miss pairs or groups of predictors that perform very well together but perform poorly as single predictors. This is similar to interviewing job candidates for a team project one by one, thereby missing groups of candidates who perform superiorly together (\"colleagues\"), but poorly on their own or with non-colleagues.\n",
    "\n",
    "In *backward elimination*, we start with all predictors and then at each step, eliminate the least useful predictor (according to statistical significance). The algorithm stops when all the remaining predictors have significant contributions. The weakness of this algorithm is that computing the initial model with all predictors can be time-consuming and unstable. *Stepwise regression* is like forward selection except that at each step, we consider dropping predictors that are not statistically significant, as in backward elimination.\n",
    "\n",
    "There is currently no support in scikit-learn or statsmodels for stepwise regression. It is however straightforward to implement such an approach in a few lines of code (see `src/dmutils/feature_selection.py` module). The below table shows the result of backward elimination for the Toyota Corolla example. The chosen eight-predictor model is identical to the best eight-predictor model chosen by the exhaustive search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d238f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables: Age_08_04, KM, HP, Met_Color, Automatic, CC, Doors, Quarterly_Tax, Weight, Fuel_Type_Diesel, Fuel_Type_Petrol\n",
      "Start: score=10422.30\n",
      "Step: score=10420.33, remove=CC\n",
      "Step: score=10418.79, remove=Met_Color\n",
      "Step: score=10417.29, remove=Doors\n",
      "Step: score=10417.29, remove=None\n",
      "LinearRegression()\n",
      "\n",
      "Regression statistics\n",
      "\n",
      "                      Mean Error (ME) : 103.3045\n",
      "       Root Mean Squared Error (RMSE) : 1314.4844\n",
      "            Mean Absolute Error (MAE) : 1016.8875\n",
      "          Mean Percentage Error (MPE) : -0.2700\n",
      "Mean Absolute Percentage Error (MAPE) : 8.9984\n"
     ]
    }
   ],
   "source": [
    "def train_model(variables):\n",
    "    model = LinearRegression()\n",
    "    model.fit(train_X[variables], train_y)\n",
    "    return model\n",
    "\n",
    "def score_model(model, variables):\n",
    "    return AIC_score(train_y, model.predict(train_X[variables]), model)\n",
    "\n",
    "\n",
    "all_variables = train_X.columns\n",
    "best_model, best_variables = backward_elimination(all_variables, train_model, score_model,\n",
    "                                                  verbose=True)\n",
    "print(best_model)\n",
    "\n",
    "regression_summary(valid_y, best_model.predict(valid_X[best_variables]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa485260",
   "metadata": {},
   "source": [
    "In this example, forward selection as well as stepwise regression also end up with this same eight-predictor model. This need not be the case with other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7bf69af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables: Age_08_04, KM, HP, Met_Color, Automatic, CC, Doors, Quarterly_Tax, Weight, Fuel_Type_Diesel, Fuel_Type_Petrol\n",
      "Start: score=11565.07, constant\n",
      "Step: score=10689.71, add Age_08_04\n",
      "Step: score=10597.91, add HP\n",
      "Step: score=10506.08, add Weight\n",
      "Step: score=10445.17, add KM\n",
      "Step: score=10435.58, add Quarterly_Tax\n",
      "Step: score=10419.93, add Fuel_Type_Petrol\n",
      "Step: score=10418.10, add Fuel_Type_Diesel\n",
      "Step: score=10417.29, add Automatic\n",
      "Step: score=10417.29, add None\n",
      "['Age_08_04', 'HP', 'Weight', 'KM', 'Quarterly_Tax', 'Fuel_Type_Petrol', 'Fuel_Type_Diesel', 'Automatic']\n"
     ]
    }
   ],
   "source": [
    "# The initial model is the constant model - this requires special handling\n",
    "# in train_model and score_model\n",
    "def train_model(variables):\n",
    "    if len(variables) == 0:\n",
    "        return None\n",
    "    model = LinearRegression()\n",
    "    model.fit(train_X[variables], train_y)\n",
    "    return model\n",
    "\n",
    "def score_model(model, variables):\n",
    "    if len(variables) == 0:\n",
    "        return AIC_score(train_y, [train_y.mean()] * len(train_y), model, df=1)\n",
    "    return AIC_score(train_y, model.predict(train_X[variables]), model)\n",
    "\n",
    "best_model, best_variables = forward_selection(train_X.columns, train_model,\n",
    "                                               score_model, verbose=True)\n",
    "print(best_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99136c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables: Age_08_04, KM, HP, Met_Color, Automatic, CC, Doors, Quarterly_Tax, Weight, Fuel_Type_Diesel, Fuel_Type_Petrol\n",
      "Start: score=11565.07, constant\n",
      "Step: score=10689.71, add Age_08_04\n",
      "Step: score=10597.91, add HP\n",
      "Step: score=10506.08, add Weight\n",
      "Step: score=10445.17, add KM\n",
      "Step: score=10435.58, add Quarterly_Tax\n",
      "Step: score=10419.93, add Fuel_Type_Petrol\n",
      "Step: score=10418.10, add Fuel_Type_Diesel\n",
      "Step: score=10417.29, add Automatic\n",
      "Step: score=10417.29, unchanged None\n",
      "['Age_08_04', 'HP', 'Weight', 'KM', 'Quarterly_Tax', 'Fuel_Type_Petrol', 'Fuel_Type_Diesel', 'Automatic']\n"
     ]
    }
   ],
   "source": [
    "best_model, best_variables = stepwise_selection(train_X.columns, train_model,\n",
    "                                                score_model, verbose=True)\n",
    "print(best_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272743a2",
   "metadata": {},
   "source": [
    "Once one or more promising models are selected, we run them to evaluate their validation predictive performance. For example, the above table shows the validation performance of the 8-predictor model, which turns out to be only very slightly better than the 10-predictor model in terms of validation metrics. It is possible there may an even smaller model, performing only slightly worse, that is preferable from a parsimony standpoint.\n",
    "\n",
    "Finally, additional ways to reduce the dimension of the data are by using principal components and regression trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afeb780",
   "metadata": {},
   "source": [
    "### Regularization (Shrinkage Models)\n",
    "\n",
    "Selecting a subset of predictors is equivalent to setting some of the model coefficients to zero. This approach creates an interpretable result - we know which predictors were dropped and which are retained. A more flexible alternative, called regularization or shrinkage, \"shrinks\" the coefficients toward zero. Recall that adjusted $R^2$ incorporates a penalty according to the number of predictors $p$. Shrinkage methods also impose a penalty on the model fit, except that the penalty is not based on the $number$ of predictors but rather on some aggregation of the coefficient values (typically predictors are first standardized to have the same scale).\n",
    "\n",
    "The reasoning behind constraining the magnitude of the $\\hat{\\beta}$ coefficients is that highly correlated predictors will tend to exhibit coefficients with high standard errors, since small changes in the training data might radically shift which of the correlated predictors gets emphasized. This instability (high standard errors) leads to poor predictive power. By constraining the combined magnitude of the coefficients, this variance is reduced.\n",
    "\n",
    "The two most popular shrinkage methods are *ridge regression* and *lasso*. They differ in terms of the penalty used: In ridge regression, the penalty is based on the sum of squared coefficients\n",
    "\n",
    "<p>\n",
    "    <center>\n",
    "        $\\sum^{p}_{j = 1}\\beta_{j}^{2}$, called L2 penalty\n",
    "    </center>\n",
    "</p>\n",
    "\n",
    "whereas lasso uses the sum of absolute values\n",
    "\n",
    "<p>\n",
    "    <center>\n",
    "        $\\sum^{p}_{j = 1}|\\beta_{j}|$, called L1 penalty\n",
    "    </center>\n",
    "</p>\n",
    "\n",
    "for $p$ predictors (excluding an intercept). It turns out that the lasso penalty effectively shrinks some of the coefficients to zero, thereby resulting in a subset of predictors.\n",
    "\n",
    "Whereas in linear regression coefficients are estimated by minimizing the training data sum of squared errors (SSE), in ridge regression and lasso the coefficients are estimated by minimizing the training data SSE, subject to the penalty term being below some threshold $t$. This threshold can be set by the user, or chosen by cross-validation.\n",
    "\n",
    "In Python, regularized linear regression can be run using methods *Lasso* and *Ridge* in `sklearn.linear_model`.The penalty parameter $\\alpha$ determines the threshold (a typical default value is $\\alpha$ = 1; note that $\\alpha$ = 0 means no penalty and yields ordinary linear regression). The methods `LassoCV`, `RidgeCV`, and `BayesianRidge` offer the added benefit of automatically selecting the penalty parameter. `LassoCV` and `RidgeCV` use cross-validation to determine optimal values for the penalty parameter. A different approach is used in `BayesianRidge` which uses an iterative approach to derive the penalty parameter from the whole training set. Remember to set argument `normalize=True` or otherwise normalize the data before applying regularized regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17f48da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso\n",
      "\n",
      "Regression statistics\n",
      "\n",
      "                      Mean Error (ME) : 120.6311\n",
      "       Root Mean Squared Error (RMSE) : 1332.2752\n",
      "            Mean Absolute Error (MAE) : 1021.5286\n",
      "          Mean Percentage Error (MPE) : -0.2364\n",
      "Mean Absolute Percentage Error (MAPE) : 9.0115\n",
      "\n",
      "Lasso CV\n",
      "\n",
      "Regression statistics\n",
      "\n",
      "                      Mean Error (ME) : 145.1571\n",
      "       Root Mean Squared Error (RMSE) : 1397.9428\n",
      "            Mean Absolute Error (MAE) : 1052.4649\n",
      "          Mean Percentage Error (MPE) : -0.2966\n",
      "Mean Absolute Percentage Error (MAPE) : 9.2918\n",
      "\n",
      "Lasso-CV chosen regularization:  3.5138446691310605\n",
      "['-140.37', '-0.02', '33.87', '0.00', '69.44', '0.00', '0.00', '2.71', '12.43', '-0.00', '0.00']\n",
      "\n",
      "Ridge\n",
      "\n",
      "Regression statistics\n",
      "\n",
      "                      Mean Error (ME) : 154.3286\n",
      "       Root Mean Squared Error (RMSE) : 1879.7426\n",
      "            Mean Absolute Error (MAE) : 1353.2735\n",
      "          Mean Percentage Error (MPE) : -2.3897\n",
      "Mean Absolute Percentage Error (MAPE) : 11.1309\n",
      "\n",
      "Bayesian Ridge\n",
      "\n",
      "Regression statistics\n",
      "\n",
      "                      Mean Error (ME) : 105.5382\n",
      "       Root Mean Squared Error (RMSE) : 1313.0217\n",
      "            Mean Absolute Error (MAE) : 1017.2356\n",
      "          Mean Percentage Error (MPE) : -0.2703\n",
      "Mean Absolute Percentage Error (MAPE) : 9.0012\n",
      "\n",
      "Bayesian ridge chosen regularization:  0.004622833439968832\n"
     ]
    }
   ],
   "source": [
    "print(\"Lasso\")\n",
    "lasso = Lasso(normalize=True, alpha=1)\n",
    "lasso.fit(train_X, train_y)\n",
    "regression_summary(valid_y, lasso.predict(valid_X))\n",
    "print()\n",
    "\n",
    "print(\"Lasso CV\")\n",
    "lasso_cv = LassoCV(normalize=True, cv=5)\n",
    "lasso_cv.fit(train_X, train_y)\n",
    "regression_summary(valid_y, lasso_cv.predict(valid_X))\n",
    "print()\n",
    "print(\"Lasso-CV chosen regularization: \", lasso_cv.alpha_)\n",
    "print([\"{:.2f}\".format(coef) for coef in lasso_cv.coef_])\n",
    "print()\n",
    "\n",
    "print(\"Ridge\")\n",
    "ridge = Ridge(normalize=True, alpha=1)\n",
    "ridge.fit(train_X, train_y)\n",
    "regression_summary(valid_y, ridge.predict(valid_X))\n",
    "print()\n",
    "\n",
    "print(\"Bayesian Ridge\")\n",
    "bayesian_ridge = BayesianRidge(normalize=True)\n",
    "bayesian_ridge.fit(train_X, train_y)\n",
    "regression_summary(valid_y, bayesian_ridge.predict(valid_X))\n",
    "print()\n",
    "alpha = bayesian_ridge.lambda_ / bayesian_ridge.alpha_\n",
    "print(\"Bayesian ridge chosen regularization: \", alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ea1cb6",
   "metadata": {},
   "source": [
    "The above code applies ridge and lasso regression to the Toyota Corolla example. We see that in this case the model performance of the optimized Lasso regression is slightly worse than for normal linear regression. Looking at the coefficients, we see that the lasso approach lead to a model with 6 predictors (Age_08_04, KM, HP, Automatic, Quarterly_Tax, Weight). The regularization parameter derived in `BayesianRidge` regression is very small, which indicates that this dataset does not benefit from regularization. The real strength of these methods becomes more evident when the dataset contains a large number of predictors with high correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77353177",
   "metadata": {},
   "source": [
    "### Using Statmodels\n",
    "\n",
    "An alternative to scikit’s `LinearRegression` is method `sm.ols` in `statmodels`. The latter produces a more extensive output of the statistical properties of the model, suitable for non-predictive tasks such as statistical inference. The following table shows the same model for price vs. car attributes run using `sm.ols`. For regularization, use `statmodels` method `OLS.fit_regularized`. Set argument `L1_wt=0` for ridge regression and `L1_wt=1` for lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79c14fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>Price</td>      <th>  R-squared:         </th> <td>   0.856</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.854</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   319.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 25 Aug 2021</td> <th>  Prob (F-statistic):</th> <td>1.73e-239</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:07:26</td>     <th>  Log-Likelihood:    </th> <td> -5198.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   600</td>      <th>  AIC:               </th> <td>1.042e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   588</td>      <th>  BIC:               </th> <td>1.047e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>        <td>-1319.3544</td> <td> 1728.427</td> <td>   -0.763</td> <td> 0.446</td> <td>-4713.997</td> <td> 2075.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Age_08_04</th>        <td> -140.7488</td> <td>    5.142</td> <td>  -27.374</td> <td> 0.000</td> <td> -150.847</td> <td> -130.650</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>KM</th>               <td>   -0.0178</td> <td>    0.002</td> <td>   -7.286</td> <td> 0.000</td> <td>   -0.023</td> <td>   -0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HP</th>               <td>   36.1034</td> <td>    5.321</td> <td>    6.785</td> <td> 0.000</td> <td>   25.653</td> <td>   46.554</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Met_Color</th>        <td>   84.2818</td> <td>  127.005</td> <td>    0.664</td> <td> 0.507</td> <td> -165.158</td> <td>  333.721</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Automatic</th>        <td>  416.7820</td> <td>  259.794</td> <td>    1.604</td> <td> 0.109</td> <td>  -93.454</td> <td>  927.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CC</th>               <td>    0.0177</td> <td>    0.099</td> <td>    0.179</td> <td> 0.858</td> <td>   -0.177</td> <td>    0.213</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Doors</th>            <td>  -50.6579</td> <td>   65.187</td> <td>   -0.777</td> <td> 0.437</td> <td> -178.686</td> <td>   77.371</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Quarterly_Tax</th>    <td>   13.6253</td> <td>    2.518</td> <td>    5.411</td> <td> 0.000</td> <td>    8.680</td> <td>   18.571</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Weight</th>           <td>   13.0387</td> <td>    1.602</td> <td>    8.140</td> <td> 0.000</td> <td>    9.893</td> <td>   16.185</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fuel_Type_Diesel</th> <td> 1066.4647</td> <td>  527.285</td> <td>    2.023</td> <td> 0.044</td> <td>   30.872</td> <td> 2102.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fuel_Type_Petrol</th> <td> 2310.2495</td> <td>  521.045</td> <td>    4.434</td> <td> 0.000</td> <td> 1286.914</td> <td> 3333.585</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>62.422</td> <th>  Durbin-Watson:     </th> <td>   1.899</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 366.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.186</td> <th>  Prob(JB):          </th> <td>3.27e-80</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.808</td> <th>  Cond. No.          </th> <td>2.20e+06</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.2e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  Price   R-squared:                       0.856\n",
       "Model:                            OLS   Adj. R-squared:                  0.854\n",
       "Method:                 Least Squares   F-statistic:                     319.0\n",
       "Date:                Wed, 25 Aug 2021   Prob (F-statistic):          1.73e-239\n",
       "Time:                        23:07:26   Log-Likelihood:                -5198.1\n",
       "No. Observations:                 600   AIC:                         1.042e+04\n",
       "Df Residuals:                     588   BIC:                         1.047e+04\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "Intercept        -1319.3544   1728.427     -0.763      0.446   -4713.997    2075.288\n",
       "Age_08_04         -140.7488      5.142    -27.374      0.000    -150.847    -130.650\n",
       "KM                  -0.0178      0.002     -7.286      0.000      -0.023      -0.013\n",
       "HP                  36.1034      5.321      6.785      0.000      25.653      46.554\n",
       "Met_Color           84.2818    127.005      0.664      0.507    -165.158     333.721\n",
       "Automatic          416.7820    259.794      1.604      0.109     -93.454     927.018\n",
       "CC                   0.0177      0.099      0.179      0.858      -0.177       0.213\n",
       "Doors              -50.6579     65.187     -0.777      0.437    -178.686      77.371\n",
       "Quarterly_Tax       13.6253      2.518      5.411      0.000       8.680      18.571\n",
       "Weight              13.0387      1.602      8.140      0.000       9.893      16.185\n",
       "Fuel_Type_Diesel  1066.4647    527.285      2.023      0.044      30.872    2102.057\n",
       "Fuel_Type_Petrol  2310.2495    521.045      4.434      0.000    1286.914    3333.585\n",
       "==============================================================================\n",
       "Omnibus:                       62.422   Durbin-Watson:                   1.899\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              366.046\n",
       "Skew:                           0.186   Prob(JB):                     3.27e-80\n",
       "Kurtosis:                       6.808   Cond. No.                     2.20e+06\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.2e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run a linear regression of Price on the remaining 11 predictors in the\n",
    "# training set\n",
    "train_df = train_X.join(train_y)\n",
    "\n",
    "predictors = train_X.columns\n",
    "formula = \"Price ~ \" + \" + \".join(predictors)\n",
    "\n",
    "car_lm = sm.ols(formula=formula, data=train_df).fit()\n",
    "car_lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c765a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
